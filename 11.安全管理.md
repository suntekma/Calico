# 11.基础认证策略

开始之前
为网格中的所有服务启用双向 TLS 认证
为一个命名空间中的所有服务启用双向 TLS
为单个服务 httpbin.bar 启用双向 TLS
同时使用命名空间层级和服务层级的策略
设置终端用户认证
清除



- 使用认证策略设置双向 TLS。

- 使用认证策略进行终端用户认证。

开始之前
- 理解 Istio 认证策略和相关的双向 TLS 认证概念。

- 拥有一个安装好 Istio 的 Kubernetes 集群，并且全局双向 TLS 处于禁用状态(可使用安装步骤中提供的示例配置 install/kubernetes/istio.yaml，或者使用 Helm 设置 global.mtls.enabled 为 false)。

- 为了演示，需要创建两个命名空间 foo 和 bar，并且在两个空间中都部署带有 sidecar 的 httpbin 应用和带 sidecar 的 sleep 应用。同时，运行另外一份不带有 sidecar 的 httpbin 和 sleep 应用(为了保证独立性，在 legacy 命名空间中运行它们)。在一个常规系统中，一个服务可以是其它服务的 服务端 (接收流量)，同时也可以是另外一些服务的 客户端 。为了简单起见，在这个演示中，我们只使用 sleep 作为客户端，使用 httpbin 作为服务端。
```
$ kubectl create ns foo
$ kubectl apply -f <(istioctl kube-inject -f samples/httpbin/httpbin.yaml) -n foo
$ kubectl apply -f <(istioctl kube-inject -f samples/sleep/sleep.yaml) -n foo
$ kubectl create ns bar
$ kubectl apply -f <(istioctl kube-inject -f samples/httpbin/httpbin.yaml) -n bar
$ kubectl apply -f <(istioctl kube-inject -f samples/sleep/sleep.yaml) -n bar
$ kubectl create ns legacy
$ kubectl apply -f samples/httpbin/httpbin.yaml -n legacy
$ kubectl apply -f samples/sleep/sleep.yaml -n legacy
```
- 通过从任意客户端(例如 sleep.foo、sleep.bar 和 sleep.legacy) 向任意服务端 (httpbin.foo、 httpbin.bar 或 httpbin.legacy) 发送 HTTP 请求(可以使用 curl 命令)来验证以上设置。所有请求都应该成功进行并且返回的 HTTP 状态码为 200。

以下是一个检查从 sleep.bar 到 httpbin.foo 可达性的命令示例：
```
$ kubectl exec $(kubectl get pod -l app=sleep -n bar -o jsonpath={.items..metadata.name}) -c sleep -n bar -- curl http://httpbin.foo:8000/ip -s -o /dev/null -w "%{http_code}\n"
200
```
以下单行命令可以方便对所有客户端服务端组合进行检查：
```
$ for from in "foo" "bar" "legacy"; do for to in "foo" "bar" "legacy"; do kubectl exec $(kubectl get pod -l app=sleep -n ${from} -o jsonpath={.items..metadata.name}) -c sleep -n ${from} -- curl http://httpbin.${to}:8000/ip -s -o /dev/null -w "sleep.${from} to httpbin.${to}: %{http_code}\n"; done; done
sleep.foo to httpbin.foo: 200
sleep.foo to httpbin.bar: 200
sleep.foo to httpbin.legacy: 200
sleep.bar to httpbin.foo: 200
sleep.bar to httpbin.bar: 200
sleep.bar to httpbin.legacy: 200
sleep.legacy to httpbin.foo: 200
sleep.legacy to httpbin.bar: 200
sleep.legacy to httpbin.legacy: 200
```
如果你在 istio-proxy 容器中安装了 curl ，你也可以验证从 proxy 到 httpbin 服务的可达性：
```
$ kubectl exec $(kubectl get pod -l app=sleep -n bar -o jsonpath={.items..metadata.name}) -c istio-proxy -n bar -- curl http://httpbin.foo:8000/ip -s -o /dev/null -w "%{http_code}\n"
200
```
最后且重要的是，验证系统目前不存在认证策略:
```
$ kubectl get meshpolicies.authentication.istio.io
No resources found.

$ kubectl get policies.authentication.istio.io --all-namespaces
No resources found.

$ kubectl get destinationrules.networking.istio.io --all-namespaces
```
你可能看到一些策略和/或由 Istio 安装时自动添加的目的地规则，具体取决于所选的安装模式。但是在 foo 、bar 和 legacy 命名空间中不应该有任何的策略或规则。

### 为网格中的所有服务启用双向 TLS 认证
你可以提交如下 网格认证策略 和目的地规则为网格中所有服务启用双向 TLS 认证：
```
cat <<EOF | istioctl create -f -
apiVersion: "authentication.istio.io/v1alpha1"
kind: "MeshPolicy"
metadata:
  name: "default"
spec:
  peers:
  - mtls: {}
EOF

cat <<EOF | istioctl create -f -
apiVersion: "networking.istio.io/v1alpha3"
kind: "DestinationRule"
metadata:
  name: "default"
spec:
  host: "*.local"
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
EOF
```
- 网格范围内的认证策略名称必须是 default；所有其它名字的策略都会被拒绝和忽视。另外注意 CRD 类型是 MeshPolicy，它不同于命名空间范围内或服务范围内的策略类型 (Policy)。
- 另一方面，目的地规则可以是任意名字，也可以存在于任意命名空间。为了保持一致性，我们在本示例中也将其命名为 default 并且使其仅存在于 default 命名空间中。
- 目的地规则中形如 *.local 的宿主名称只匹配网格中以 local 结尾的服务。
- 当处于 ISTIO_MUTUAL TLS 模式， Istio 会依据内部实现机制设置密钥和证书的路径(例如 clientCertificate 、 privateKey 和 caCertificates)。
- 如果你想要为某一特定服务定义目的地规则，那么 TLS 相关的设置也必须被复制到新规则中。
这些认证策略和目的地规则有效地配置了所有服务的 sidecars，使服务在双向 TLS 模式下分别进行接收和发送请求。但是这对于没有 sidecar 的服务并不适用，例如上文中创建的 httpbin.legacy 和 sleep.legacy 服务。如果你运行上文中提供的测试命令，你会发现从 sleep.legacy 到 httpbin.foo 和 httpbin.bar 的请求开始出现失败现象，这是由于虽然在服务端启用了双向 TLS 认证，但 sleep.legacy 并没有 sidecar 来支持认证。类似的，从 sleep.foo (或 sleep.bar) 到 httpbin.legacy 的请求也会失败。
```
$ for from in "foo" "bar" "legacy"; do for to in "foo" "bar" "legacy"; do kubectl exec $(kubectl get pod -l app=sleep -n ${from} -o jsonpath={.items..metadata.name}) -c sleep -n ${from} -- curl http://httpbin.${to}:8000/ip -s -o /dev/null -w "sleep.${from} to httpbin.${to}: %{http_code}\n"; done; done
sleep.foo to httpbin.foo: 200
sleep.foo to httpbin.bar: 200
sleep.foo to httpbin.legacy: 503
sleep.bar to httpbin.foo: 200
sleep.bar to httpbin.bar: 200
sleep.bar to httpbin.legacy: 503
sleep.legacy to httpbin.foo: 000
command terminated with exit code 56
sleep.legacy to httpbin.bar: 000
command terminated with exit code 56
sleep.legacy to httpbin.legacy: 200
```
返回的 HTTP 错误码还没有统一。如果 HTTP 请求是在双向 TLS 模式下发送并且服务端只接受 HTTP 请求，则返回的错误码为 503。相反，如果请求是以纯文本的格式发送到使用双向 TLS 的服务端，则返回的错误码是 000 (同时 curl 退出码为 56，错误信息是 “failure with receiving network data”)。

为了修复从带有 sidecar 的客户端到不带 sidecar 的服务端的连接，你可以专门为这些服务端添加目的地规则来覆盖 TLS 设置。
```
cat <<EOF | istioctl create -f -
apiVersion: "networking.istio.io/v1alpha3"
kind: "DestinationRule"
metadata:
  name: "httpbin"
  namespace: "legacy"
spec:
  host: "httpbin.legacy.svc.cluster.local"
  trafficPolicy:
    tls:
      mode: DISABLE
EOF
```
重新尝试发送请求到 httpbin.legacy，一切都应该正常工作了。
```
$ for from in "foo" "bar" "legacy"; do for to in "legacy"; do kubectl exec $(kubectl get pod -l app=sleep -n ${from} -o jsonpath={.items..metadata.name}) -c sleep -n ${from} -- curl http://httpbin.${to}:8000/ip -s -o /dev/null -w "sleep.${from} to httpbin.${to}: %{http_code}\n"; done; done
sleep.foo to httpbin.legacy: 200
sleep.bar to httpbin.legacy: 200
sleep.legacy to httpbin.legacy: 200
```
当启用全局双向 TLS 认证时，这种方法也可以用来配置 Kubernetes 的 API 服务器。如下是一个示例配置：
```
cat <<EOF | istioctl create -f -
apiVersion: "networking.istio.io/v1alpha3"
kind: "DestinationRule"
metadata:
  name: "api-server"
  namespace: "default"
spec:
  host: "kubernetes.default.svc.cluster.local"
  trafficPolicy:
    tls:
      mode: DISABLE
EOF
```
对于第二个问题，从不带 sidecar 的客户端到带有 sidecar 的服务端(工作在双向 TLS 模式)的连接，唯一的选择是从双向 TLS 模式切换到 PERMISSIVE 模式，该模式允许服务端接收 HTTP 或（双向） TLS 流量。显然，这种模式会降低安全等级，推荐只在迁移过程中使用。为了这样做，你可以更改 网格策略 (在 mtls 域下增加 mode: PERMISSIVE)。推荐一种更保守的方法：仅在必要的情况下才为个别服务创建专属策略。下面的例子演示了这种保守的方法：
```
cat <<EOF | istioctl create -f -
apiVersion: "authentication.istio.io/v1alpha1"
kind: "Policy"
metadata:
  name: "httpbin"
  namespace: "foo"
spec:
  targets:
  - name: "httpbin"
  peers:
  - mtls:
      mode: PERMISSIVE
EOF
```
从 sleep.legacy 到 httpbin.foo 的请求应当是成功的，但是到 httpbin.bar 的请求依然会失败。
```
$ kubectl exec $(kubectl get pod -l app=sleep -n legacy -o jsonpath={.items..metadata.name}) -c sleep -n legacy -- curl http://httpbin.foo:8000/ip -s -o /dev/null -w "%{http_code}\n"
200

$ kubectl exec $(kubectl get pod -l app=sleep -n legacy -o jsonpath={.items..metadata.name}) -c sleep -n legacy -- curl http://httpbin.bar:8000/ip -s -o /dev/null -w "%{http_code}\n"
000
```
在进入下一小节之前，我们需要将在本小节创建的认证策略和目的地规则移除掉。
```
$ kubectl delete meshpolicy.authentication.istio.io default
$ kubectl delete policy.authentication.istio.io -n foo --all
$ kubectl delete destinationrules.networking.istio.io default
$ kubectl delete destinationrules.networking.istio.io -n legacy --all
```
### 为一个命名空间中的所有服务启用双向 TLS
你可以为每一个命名空间单独启用双向 TLS 而不必启用全局双向 TLS。启用的步骤是类似的，只是相关的策略只在命名空间范围内有效（类型为 Policy）。
```
cat <<EOF | istioctl create -f -
apiVersion: "authentication.istio.io/v1alpha1"
kind: "Policy"
metadata:
  name: "default"
  namespace: "foo"
spec:
  peers:
  - mtls: {}
EOF
```
类似于 网格范围内的策略 ，命名空间范围内的策略必须命名为 default，并且不限定任何特定的服务（没有 targets 设置域）

- 添加相应的目的地规则：
```
cat <<EOF | istioctl create -f -
apiVersion: "networking.istio.io/v1alpha3"
kind: "DestinationRule"
metadata:
  name: "default"
  namespace: "foo"
spec:
  host: "*.foo.svc.cluster.local"
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
EOF
```
宿主名称 *.foo.svc.cluster.local 限制了只能匹配命名空间 foo 中的服务。

由于这些策略和目的地规则只对命名空间 foo 中的服务有效，你应该看到只有从不带 sidecar 的客户端 (sleep.legacy) 到 httpbin.foo 的请求会出现失败。
```
$ for from in "foo" "bar" "legacy"; do for to in "foo" "bar" "legacy"; do kubectl exec $(kubectl get pod -l app=sleep -n ${from} -o jsonpath={.items..metadata.name}) -c sleep -n ${from} -- curl http://httpbin.${to}:8000/ip -s -o /dev/null -w "sleep.${from} to httpbin.${to}: %{http_code}\n"; done; done
sleep.foo to httpbin.foo: 200
sleep.foo to httpbin.bar: 200
sleep.foo to httpbin.legacy: 200
sleep.bar to httpbin.foo: 200
sleep.bar to httpbin.bar: 200
sleep.bar to httpbin.legacy: 200
sleep.legacy to httpbin.foo: 000
command terminated with exit code 56
sleep.legacy to httpbin.bar: 200
sleep.legacy to httpbin.legacy: 200
```
### 为单个服务 httpbin.bar 启用双向 TLS
你也可以为某个特定的服务设置认证策略和目的地规则。执行以下命令只为 httpbin.bar 服务新增一项策略。
```
cat <<EOF | istioctl create -n bar -f -
apiVersion: "authentication.istio.io/v1alpha1"
kind: "Policy"
metadata:
  name: "httpbin"
spec:
  targets:
  - name: httpbin
  peers:
  - mtls: {}
EOF
```
同时增加目的地规则：
```
cat <<EOF | istioctl create -n bar -f -
apiVersion: "networking.istio.io/v1alpha3"
kind: "DestinationRule"
metadata:
  name: "httpbin"
spec:
  host: "httpbin.bar.svc.cluster.local"
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
EOF
```
在这个例子中，我们 不 在 metadata 中指定命名空间而是放在命令行 (-n bar) 中。它们的效果是一样的。 对于认证策略和目的地规则的名称并没有任何限定。在本例中为了简单，使用服务本身的名称为策略和规则命名。

同样地，运行上文中提供的测试命令。和预期一致，从 sleep.legacy 到 httpbin.bar 的请求因为同样的原因开始出现失败。
```
...
sleep.legacy to httpbin.bar: 000
command terminated with exit code 56
```
如果在命名空间 bar 中还存在其他服务，我们会发现目标为这些服务的流量不会受到影响。验证这一行为有两种方法：一种是加入更多服务；另一种是把这一策略限制到某个端口。这里我们展示第二种方法：
```
cat <<EOF | istioctl replace -n bar -f -
apiVersion: "authentication.istio.io/v1alpha1"
kind: "Policy"
metadata:
  name: "httpbin"
spec:
  targets:
  - name: httpbin
    ports:
    - number: 1234
  peers:
  - mtls:
EOF
```
同时对目的地规则做出相应的改变：
```
cat <<EOF | istioctl replace -n bar -f -
apiVersion: "networking.istio.io/v1alpha3"
kind: "DestinationRule"
metadata:
  name: "httpbin"
spec:
  host: httpbin.bar.svc.cluster.local
  trafficPolicy:
    tls:
      mode: DISABLE
    portLevelSettings:
    - port:
        number: 1234
      tls:
        mode: ISTIO_MUTUAL
EOF
```
这项新的策略只作用于 httpbin 服务的 1234 端口上。结果是，双向 TLS 在端口 8000 上（又）被禁用并且从 sleep.legacy 发出的请求会恢复工作。
```
$ kubectl exec $(kubectl get pod -l app=sleep -n legacy -o jsonpath={.items..metadata.name}) -c sleep -n legacy -- curl http://httpbin.bar:8000/ip -s -o /dev/null -w "%{http_code}\n"
200
```
### 同时使用命名空间层级和服务层级的策略
假设我们已经为命名空间 foo 中所有的服务添加了启用双向 TLS 的命名空间层级的策略并且观察到从 sleep.legacy 到 httpbin.foo 的请求都失败了（见上文）。现在专门为 httpbin 服务添加额外的策略来禁用双向 TLS （peers 域留空）：
```
cat <<EOF | istioctl create -n foo -f -
apiVersion: "authentication.istio.io/v1alpha1"
kind: "Policy"
metadata:
  name: "example-3"
spec:
  targets:
  - name: httpbin
EOF
```
另外添加对应的目的地规则：
```
cat <<EOF | istioctl create -n foo -f -
apiVersion: "networking.istio.io/v1alpha3"
kind: "DestinationRule"
metadata:
  name: "example-3"
spec:
  host: httpbin.foo.svc.cluster.local
  trafficPolicy:
    tls:
      mode: DISABLE
EOF
```
重新从 sleep.legacy 发送请求，我们应当看到请求成功返回的状态码（200），表明服务层级的策略覆盖了命名空间层级的策略。
```
$ kubectl exec $(kubectl get pod -l app=sleep -n legacy -o jsonpath={.items..metadata.name}) -c sleep -n legacy -- curl http://httpbin.foo:8000/ip -s -o /dev/null -w "%{http_code}\n"
200
```

### 设置终端用户认证
你需要一个有效的 JWT （与在本例中你想使用的 JWKS endpoint 相一致）。请按照这里的说明进行操作来创建一个 JWT 。你也可以在示例中使用自己的 JWT/JWKS endpoint。创建之后，在环境变量中设置相关信息。
```
$ export SVC_ACCOUNT="example@my-project.iam.gserviceaccount.com"
$ export JWKS=https://www.googleapis.com/service_accounts/v1/jwk/${SVC_ACCOUNT}
$ export TOKEN=<YOUR-TOKEN>
```
另外，为了方便，通过入口暴露 httpbin.foo 服务（详细信息参考入口任务）。
```
cat <<EOF | kubectl apply -f -
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: httpbin-ingress
  namespace: foo
  annotations:
    kubernetes.io/ingress.class: istio
spec:
  rules:
  - http:
      paths:
      - path: /headers
        backend:
          serviceName: httpbin
          servicePort: 8000
EOF
```
获取入口 IP:
```
$ export INGRESS_HOST=$(kubectl get ing -n foo -o=jsonpath='{.items[0].status.loadBalancer.ingress[0].ip}')
```
并且运行查询测试:
```
$ curl $INGRESS_HOST/headers -s -o /dev/null -w "%{http_code}\n"
200
```
现在，让我们为 httpbin.foo 添加一项策略使其必须通过用户 JWT 访问。下一步命令假设名称为 “httpbin” 的策略已经存在（如果你是按照前面小节的说明进行的操作）。你可以运行 kubectl get policies.authentication.istio.io -n foo 进行确认。如果相应资源不存在，使用 istio create （替换 istio replace ）创建资源。注意在以下策略中，对端的认证方式（双向 TLS ）也会被设置，尽管可以移除该设置同时不影响初始的认证设置。
```
cat <<EOF | istioctl replace -n foo -f -
apiVersion: "authentication.istio.io/v1alpha1"
kind: "Policy"
metadata:
  name: "example-3"
spec:
  targets:
  - name: httpbin
  peers:
  - mtls:
  origins:
  - jwt:
      issuer: $SVC_ACCOUNT
      jwksUri: $JWKS
  principalBinding: USE_ORIGIN
EOF
```
使用上面小节中同样的 curl 命令进行测试时会返回 401 错误状态码，这是因为服务端需要 JWT 进行认证但请求端并没有提供：
```
$ curl $INGRESS_HOST/headers -s -o /dev/null -w "%{http_code}\n"
401
```
在请求中附加上面操作生成的 token ，然后执行请求就会返回成功信息：
```
$ curl --header "Authorization: Bearer $TOKEN" $INGRESS_HOST/headers -s -o /dev/null -w "%{http_code}\n"
200
```
你也可以尝试修改 token 或 policy （例如改变 issuer、audiences、expiry date 等信息）来观察 JWT 验证的其它方面信息。

### 清除
清除所有资源。
```
$ kubectl delete ns foo bar legacy
```

___


# 测试双向 TLS
开始之前
- 检查 Istio 双向 TLS 认证的配置
- 检查 Citadel
- 检查服务配置
- 校验密钥和证书的安装情况
- 测试认证配置
- 清理


验证 Istio 双向 TLS 认证配置
手动对认证功能进行测试
### 开始之前
本任务假设已有一个 Kubernetes 集群：

安装启用全局双向 TLS 认证功能的 Istio：
```
$ kubectl apply -f install/kubernetes/istio-demo-auth.yaml
```
或者

使用 Helm 进行安装，设置 global.mtls.enabled 为 true.

从 Istio 0.7 开始，可以使用认证策略来给命名空间中全部/部分服务配置双向 TLS 功能。（在所有命名空间中重复此操作，就相当于全局配置了）。这部分内容可参考认证策略任务
接下来进行演示应用的部署，首先是注入 Envoy sidecar 的 httpbin 以及 sleep。为简单起见，我们将演示应用安装到 default 命名空间。如果想要部署到其他命名空间，可以在下一节的示例命令中加入 -n yournamespace。

如果使用的是手工 Sidecar 注入，可使用如下命令：
```
$ kubectl apply -f <(istioctl kube-inject -f samples/httpbin/httpbin.yaml)
$ kubectl apply -f <(istioctl kube-inject -f samples/sleep/sleep.yaml)
```
如果集群设置了自动注入 Sidecar，就只需要简单的使用 kubectl 就可以完成部署了。
```
$ kubectl apply -f samples/httpbin/httpbin.yaml
$ kubectl apply -f samples/sleep/sleep.yaml
```
## 检查 Istio 双向 TLS 认证的配置
## 检查 Citadel
检查集群内是否运行了 Citadel：
```
$ kubectl get deploy -l istio=citadel -n istio-system
NAME            DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
istio-citadel   1         1         1            1           1m
```
如果 “AVAILABLE” 列值为 1，则说明 Citadel 已经成功运行。

### 检查服务配置
检查安装模式。如果缺省启用了双向 TLS（也就是在安装 Istio 的时候使用了 istio-demo-auth.yaml），会在 Configmap 中看到未被注释的 authPolicy: MUTUAL_TLS 一行：
```
$ kubectl get configmap istio -o yaml -n istio-system | grep authPolicy | head -1
```
检查认证策略。双向 TLS 的策略还能够以服务为单位进行启用（或停用）。如果存在仅对部分服务生效的策略，那么这部分服务原有的来自 Configmap 的策略就会被覆盖。不幸的是，目前没有快速的方法能够方便的获取某个服务的对应策略，只能在命名空间内获取所有策略。
```
$ kubectl get policies.authentication.istio.io -n default -o yaml
```
检查目标规则。从 Istio 0.8 开始，会使用目标规则的流量策略来对客户端进行配置，决定是否使用双向 TLS。为了向后兼容，缺省流量策略来自 Configmap 中的标志（也就是说，如果设置了 authPolicy: MUTUAL_TLS，那么缺省流量策略也会是 MUTUAL_TLS ）。如果使用针对部分服务的认证策略覆盖了原有配置，那么就要通过目标规则来实现了。跟认证策略类似，验证这一设置的方法也是需要通过获取全部规则的方式来进行：
```
$ kubectl get destinationrules.networking.istio.io --all-namespaces -o yaml
```
注意目标规则的范围不仅限于单一命名空间，所以需要验证所有命名空间的规则。

### 校验密钥和证书的安装情况
为了完成双向 TLS 认证功能，Istio 会自动在所有 Sidecar 容器中安装必要的密钥和证书。
```
$ kubectl exec $(kubectl get pod -l app=httpbin -o jsonpath={.items..metadata.name}) -c istio-proxy -- ls /etc/certs
cert-chain.pem
key.pem
root-cert.pem
```
cert-chain.pem 是 Envoy 的证书，会在需要的时候提供给对端。而 key.pem 就是 Envoy 的私钥，和 cert-chain.pem 中的证书相匹配。root-cert.pem 是用于证书校验的根证书。这个例子中，我们集群中只部署了一个 Citadel，所以所有的 Envoy 共用同一个 root-cert.pem。

是用 openssl 工具来检查证书的有效性（当前时间应该处于 Not Before 和 Not After 之间）
```
$ kubectl exec $(kubectl get pod -l app=httpbin -o jsonpath={.items..metadata.name}) -c istio-proxy -- cat /etc/certs/cert-chain.pem | openssl x509 -text -noout  | grep Validity -A 2
Validity
        Not Before: May 17 23:02:11 2018 GMT
        Not After : Aug 15 23:02:11 2018 GMT
```
还可以查看一下客户端证书的 SAN（Subject Alternative Name）

$ kubectl exec $(kubectl get pod -l app=httpbin -o jsonpath={.items..metadata.name}) -c istio-proxy -- cat /etc/certs/cert-chain.pem | openssl x509 -text -noout  | grep 'Subject Alternative Name' -A 1
        X509v3 Subject Alternative Name:
            URI:spiffe://cluster.local/ns/default/sa/default

请参阅 Istio 认证 一节，可以了解更多服务认证方面的内容。

### 测试认证配置
假设双向 TLS 认证正确启用，在两个注入了 Envoy sidecar 的服务之间的通信应该不会受到影响。然而如果从没有注入 Sidecar 的 Pod 发起连接，或者直接从 Sidecar 发起没有指定客户端证书的连接，就无法访问服务了。下面的例子就演示了这种情况：

1.从 sleep 容器访问 httpbin 服务应该可以成功（返回 200）
```
$ kubectl exec $(kubectl get pod -l app=sleep -o jsonpath={.items..metadata.name}) -c sleep -- curl httpbin:8000/headers -o /dev/null -s -w '%{http_code}\n'
200
```
2.如果从 sleep 的 proxy 容器中访问 httpbin 服务，就会导致失败
```
$ kubectl exec $(kubectl get pod -l app=sleep -o jsonpath={.items..metadata.name}) -c istio-proxy -- curl httpbin:8000/headers -o /dev/null -s -w '%{http_code}\n'
000
command terminated with exit code 56

$ kubectl exec $(kubectl get pod -l app=sleep -o jsonpath={.items..metadata.name}) -c istio-proxy -- curl https://httpbin:8000/headers -o /dev/null -s -w '%{http_code}\n'
000
command terminated with exit code 77
```
3.接下来，如果请求中提供了客户端证书，那么这次请求就会成功
```
$ kubectl exec $(kubectl get pod -l app=sleep -o jsonpath={.items..metadata.name}) -c istio-proxy -- curl https://httpbin:8000/headers -o /dev/null -s -w '%{http_code}\n' --key /etc/certs/key.pem --cert /etc/certs/cert-chain.pem --cacert /etc/certs/root-cert.pem -k
200
```
Istio 使用 Kubernetes service accounts 作为服务的认证基础，Service account 提供了比服务名称更强的安全性（参考 Identity 获取更多信息）。Istio 中使用的证书不包含服务名，而 curl 需要用这个信息来检查服务认证。因此就需要给 curl 命令加上 -k 参数，在对服务器所出示的证书校验的时候，停止对服务器名称（例如 httpbin.ns.svc.cluster.local ）的验证。

4.来自没有 Sidecar 的 Pod。可以重新部署另外一个 sleep 应用
```
$ kubectl create ns legacy
$ kubectl apply -f samples/sleep/sleep.yaml -n legacy
```
5.等待 Pod 状态变为 Running，在其中发起类似的 curl 命令。由于这个 Pod 没有 Sidecar 协助完成 TLS 通信，因此这一请求会失败。
```
$ kubectl exec $(kubectl get pod -l app=sleep -o jsonpath={.items..metadata.name} -n legacy) -c sleep -n legacy -- curl httpbin.default:8000/headers -o /dev/null -s -w '%{http_code}\n'
000
command terminated with exit code 56
```
### 清理
```
$ kubectl delete --ignore-not-found=true -f samples/httpbin/httpbin.yaml
$ kubectl delete --ignore-not-found=true -f samples/sleep/sleep.yaml
$ kubectl delete --ignore-not-found=true ns legacy
```
___


# 基于角色的访问控制
### 开始之前
#### 启用 Istio 授权
### 命名空间级的访问控制
- 清除命名空间级的访问控制
#### 服务级的访问控制
- 第一步，允许到 productpage 服务的访问
- 第二步，允许对 details 和 reviews 服务的访问。
- 第三步，允许对 ratings 服务的访问
清理

在服务网格中为服务进行授权控制（基于角色的访问控制）时，会涉及到本例中包含的一系列操作。在授权一节中讲述了更多这方面的内容，并且还有一个基本的 Istio 安全方面的教程。

### 开始之前
本文活动开始之前，我们有如下假设：

- 具有对授权概念的了解。

- 在 Istio 中遵循快速入门的步骤 启用了认证功能，这个教程对双向 TLS 有依赖，因此要在安装步骤中启用双向 TLS 认证。

- 部署 Bookinfo 示例应用。

- 这个任务里，我们会在 Service account 的基础上启用访问控制，在网格中进行加密的认证。为了给不同的微服务以不同的访问授权，就需要建立一系列不同的 Service account，用这些账号来分别运行 Bookinfo 中的微服务。

运行命令，完成以下目的：

- 创建 Service account：bookinfo-productpage，并用这一身份部署 productpage。
- 创建 Service account：bookinfo-reviews，并用这一身份部署 reviews（注意其中包含 reviews-v2 和 reviews-v3 两个版本）。
```
$ kubectl apply -f <(istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo-add-serviceaccount.yaml)
```
如果使用的命名空间不是 default，就应改用 istioctl -n namespace ... 来指定命名空间。

- Istio 1.0 中的 RBAC 有较大更新。请确认在继续之前，已经清理了所有现存 RBAC 规则。

运行下面的命令，禁用旧的 RBAC 功能，在 1.0 中就无需这一步骤了：
```
$ kubectl delete authorization requestcontext -n istio-system
$ kubectl delete rbac handler -n istio-system
$ kubectl delete rule rbaccheck -n istio-system
```
用这个命令移除所有现存 RBAC 策略：
保存现有策略是可以的，不过需要对策略的 constraints 以及 properties 字段进行修改，参考约束和属性中的内容，了解这两个字段所支持的值。
```
$ kubectl delete servicerole --all
$ kubectl delete servicerolebinding --all
```
用浏览器打开 Bookinfo 的 productpage 页面，会看到：

- 页面左下角是 “Book Details”，其中包含了类型、页数、出版商等信息。
- 页面右下角是 “Book Reviews” 部分。
如果刷新几次，会发现 productpage 在切换使用不同的 reviews 版本（红星、黑星、无）。

### 启用 Istio 授权
运行下面的命令，为 default 命名空间启用 Istio 授权：
```
$ istioctl create -f samples/bookinfo/platform/kube/rbac/rbac-config-ON.yaml
```
如果前面已经创建了冲突的规则，应该使用 istioctl replace 替代 istioctl create。

用浏览器再次打开 productpage (http://$GATEWAY_URL/productpage)，这次会看到 RBAC: access denied。Istio 的鉴权行为是“缺省拒绝”的，也就是说必须要显式的进行授权，才能对服务进行访问。

缓存和传播可能会造成一定的延迟。

命名空间级的访问控制
Istio 的授权能力可以轻松的设置命名空间级的访问控制，只要指定命名空间内的所有（或者部分）服务可以被另一命名空间的服务访问即可。

Bookinfo 示例中，productpage、reviews、details 以及 ratings 服务被部署在 default 命名空间中，而 istio-ingressgateway 等 Istio 组件是部署在 istio-system 命名空间中的。我们可以定义一个策略，default 命名空间中所有服务，如果其 app 标签取值在 productpage、reviews、details 以及 ratings 范围之内，就可以被本命名空间内以及 istio-system 命名空间内的服务进行访问。

运行这一命令，创建一个命名空间级别的访问控制策略：
```
$ istioctl create -f samples/bookinfo/platform/kube/rbac/namespace-policy.yaml
```
这一策略完成如下任务：

- 创建名为 service-viewer 的 ServiceRole，允许访问 default 命名空间中所有 app 标签值在 productpage、reviews、details 以及 ratings 范围之内的服务。注意其中的 constraint 字段，确定了服务的 app 标签取值必须在指定范围以内：
```
apiVersion: "rbac.istio.io/v1alpha1"
kind: ServiceRole
metadata:
  name: service-viewer
  namespace: default
spec:
  rules:
  - services: ["*"]
    methods: ["GET"]
    constraints:
    - key: "destination.labels[app]"
      values: ["productpage", "details", "reviews", "ratings"]
```
- 创建 ServiceRoleBinding 对象，用来把 service-viewer 角色指派给所有 istio-system 和 default 命名空间的服务：
```
apiVersion: "rbac.istio.io/v1alpha1"
kind: ServiceRoleBinding
metadata:
  name: bind-service-viewer
  namespace: default
spec:
  subjects:
  - properties:
      source.namespace: "istio-system"
  - properties:
      source.namespace: "default"
    roleRef:
    kind: ServiceRole
    name: "service-viewer"
```
命令执行结果大致如下：
```
servicerole "service-viewer" created
servicerolebinding "bind-service-viewer" created
```
如果这时用浏览器浏览 Bookinfo 的 productpage 页面 (http://$GATEWAY_URL/productpage)，会再次看到完整的页面，包含了左下角的 “Book Details” 以及右下角的 “Book Reviews”。

缓存和传播可能会造成一定的延迟。

清除命名空间级的访问控制
在进行后续任务之前，首先移除下面的配置：
```
$ istioctl delete -f samples/bookinfo/platform/kube/rbac/namespace-policy.yaml
```
服务级的访问控制
这个任务展示了使用 Istio 授权功能配置服务级访问控制的方法。开始之前，请进行下面的确认：

已经启用 Istio 授权
已经[清除命名空间级的访问控制](清除命名空间级的访问控制：
浏览器打开 Bookinfo 的 productpage (http://$GATEWAY_URL/productpage)。会看到：RBAC: access denied。我们会在 Bookinfo 中逐步为服务加入访问许可。

第一步，允许到 productpage 服务的访问
这里我们要创建一条策略，允许外部请求通过 Ingress 浏览 productpage。

执行命令：
```
$ istioctl create -f samples/bookinfo/platform/kube/rbac/productpage-policy.yaml
```
这条策略完成以下工作：

创建 ServiceRole，命名为 productpage-viewer，允许到 productpage 服务的读取访问：
```
apiVersion: "rbac.istio.io/v1alpha1"
kind: ServiceRole
metadata:
  name: productpage-viewer
  namespace: default
spec:
  rules:
  - services: ["productpage.default.svc.cluster.local"]
    methods: ["GET"]
```
创建 ServiceRole，并命名为 productpage-viewer，将 productpage-viewer 角色赋予给所有用户和服务：
```
apiVersion: "rbac.istio.io/v1alpha1"
kind: ServiceRoleBinding
metadata:
  name: bind-productpager-viewer
  namespace: default
spec:
  subjects:
  - user: "*"
    roleRef:
    kind: ServiceRole
    name: "productpage-viewer"
```
再次浏览 Bookinfo 的 productpage (http://$GATEWAY_URL/productpage)。应该能看到 “Bookinfo Sample” 页面了。但是还会显示 Error fetching product details 和 Error fetching product reviews 的错误信息。这是因为我们还没有给 productpage 访问 details 和 reviews 服务的授权。我们接下来就修复这个问题。

缓存和传播可能会造成一定的延迟。

第二步，允许对 details 和 reviews 服务的访问。
创建一条策略，让 productpage 服务能够读取 details 和 reviews 服务。注意在开始之前中，我们给 productpage 服务创建了一个命名为 bookinfo-productpage 的 Service account，它就是 productpage 服务的认证 ID。

运行下面的命令：
```
$ istioctl create -f samples/bookinfo/platform/kube/rbac/details-reviews-policy.yaml
```
这一策略完成以下任务：

创建一个 ServiceRole，命名为 details-reviews-viewer，允许对 details 和 reviews 服务进行只读访问。
```
apiVersion: "rbac.istio.io/v1alpha1"
kind: ServiceRole
metadata:
  name: details-reviews-viewer
  namespace: default
spec:
  rules:
  - services: ["details.default.svc.cluster.local", "reviews.default.svc.cluster.local"]
    methods: ["GET"]
```
创建一个 ServiceRoleBinding 并命名为 bind-details-review，用来把 details-reviews-viewer 角色授予给 cluster.local/ns/default/sa/bookinfo-productpage（也就是 productpage 服务的 Service account）。
```
apiVersion: "rbac.istio.io/v1alpha1"
kind: ServiceRoleBinding
metadata:
  name: bind-details-reviews
  namespace: default
spec:
  subjects:
  - user: "spiffe://cluster.local/ns/default/sa/bookinfo-productpage"
    roleRef:
    kind: ServiceRole
    name: "details-reviews-viewer"
```
浏览 Bookinfo 页面 productpage (http://$GATEWAY_URL/productpage)。现在看到的 “Bookinfo Sample” 中包含了左下角的 “Book Details” 以及右下角的 “Book Reviews”。然而 “Book Reviews” 中有一条错误信息： Ratings service currently unavailable，这是因为 reviews 服务无权访问 ratings 服务，要更正这一问题，就应该给 ratings 服务授权，使其能够访问 reviews 服务。下面的步骤就会完成这一需要。

缓存和传播可能会造成一定的延迟。

第三步，允许对 ratings 服务的访问
接下来新建一条策略，允许 reviews 服务对 ratings 发起读取访问。注意，我们在开始之前步骤里为 reviews 服务创建了 Service account bookinfo-reviews，这个账号就是 reviews 服务的认证凭据。

下面的命令会创建一条允许 reviews 服务读取 ratings 服务的策略。
```
$ istioctl create -f samples/bookinfo/platform/kube/rbac/ratings-policy.yaml
```
这条策略完成以下工作：

创建 ServiceRole 命名为 ratings-viewer，这一角色允许对 ratings 服务的访问。
```
apiVersion: "rbac.istio.io/v1alpha1"
kind: ServiceRole
metadata:
  name: ratings-viewer
  namespace: default
spec:
  rules:
  - services: ["ratings.default.svc.cluster.local"]
    methods: ["GET"]
```
创建 ServiceRoleBinding，命名为 bind-ratings，将 ratings-viewer 角色指派给 cluster.local/ns/default/sa/bookinfo-reviews，给这个 Service account 授权，也就就代表了给 reviews 服务授权。
```
apiVersion: "rbac.istio.io/v1alpha1"
kind: ServiceRoleBinding
metadata:
  name: bind-ratings
  namespace: default
spec:
  subjects:
  - user: "spiffe://cluster.local/ns/default/sa/bookinfo-reviews"
    roleRef:
    kind: ServiceRole
    name: "ratings-viewer"
```
用浏览器浏览 Bookinfo 应用的 productpage (http://$GATEWAY_URL/productpage)，应该就会看到 “Book Reviews” 区域中显示红色或者黑色的评级信息。

缓存和传播可能会造成一定的延迟。

清理
### 清理 Istio 授权策略的相关配置：
```
$ istioctl delete -f samples/bookinfo/platform/kube/rbac/ratings-policy.yaml
$ istioctl delete -f samples/bookinfo/platform/kube/rbac/details-reviews-policy.yaml
$ istioctl delete -f samples/bookinfo/platform/kube/rbac/productpage-policy.yaml
```
- 或者也可以运行命令删除所有的 ServiceRole 以及 ServiceRoleBinding 资源：
```
$ kubectl delete servicerole --all
$ kubectl delete servicerolebinding --all
```
- 禁用 Istio 的授权功能：
```
$ istioctl delete -f samples/bookinfo/platform/kube/rbac/rbac-config-ON.yaml
```

___


# 基于角色的访问控制
开始之前
启用 Istio 授权
命名空间级的访问控制
清除命名空间级的访问控制
服务级的访问控制
第一步，允许到 productpage 服务的访问
第二步，允许对 details 和 reviews 服务的访问。
第三步，允许对 ratings 服务的访问
清理

在服务网格中为服务进行授权控制（基于角色的访问控制）时，会涉及到本例中包含的一系列操作。在授权一节中讲述了更多这方面的内容，并且还有一个基本的 Istio 安全方面的教程。

### 开始之前
本文活动开始之前，我们有如下假设：

具有对授权概念的了解。

在 Istio 中遵循快速入门的步骤 启用了认证功能，这个教程对双向 TLS 有依赖，因此要在安装步骤中启用双向 TLS 认证。

部署 Bookinfo 示例应用。

这个任务里，我们会在 Service account 的基础上启用访问控制，在网格中进行加密的认证。为了给不同的微服务以不同的访问授权，就需要建立一系列不同的 Service account，用这些账号来分别运行 Bookinfo 中的微服务。

运行命令，完成以下目的：

创建 Service account：bookinfo-productpage，并用这一身份部署 productpage。
创建 Service account：bookinfo-reviews，并用这一身份部署 reviews（注意其中包含 reviews-v2 和 reviews-v3 两个版本）。
```
$ kubectl apply -f <(istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo-add-serviceaccount.yaml)
```
如果使用的命名空间不是 default，就应改用 istioctl -n namespace ... 来指定命名空间。

Istio 1.0 中的 RBAC 有较大更新。请确认在继续之前，已经清理了所有现存 RBAC 规则。

运行下面的命令，禁用旧的 RBAC 功能，在 1.0 中就无需这一步骤了：
```
$ kubectl delete authorization requestcontext -n istio-system
$ kubectl delete rbac handler -n istio-system
$ kubectl delete rule rbaccheck -n istio-system
```
用这个命令移除所有现存 RBAC 策略：
保存现有策略是可以的，不过需要对策略的 constraints 以及 properties 字段进行修改，参考约束和属性中的内容，了解这两个字段所支持的值。
```
$ kubectl delete servicerole --all
$ kubectl delete servicerolebinding --all
```
用浏览器打开 Bookinfo 的 productpage 页面，会看到：

页面左下角是 “Book Details”，其中包含了类型、页数、出版商等信息。
页面右下角是 “Book Reviews” 部分。
如果刷新几次，会发现 productpage 在切换使用不同的 reviews 版本（红星、黑星、无）。

### 启用 Istio 授权
运行下面的命令，为 default 命名空间启用 Istio 授权：
```
$ istioctl create -f samples/bookinfo/platform/kube/rbac/rbac-config-ON.yaml
```
如果前面已经创建了冲突的规则，应该使用 istioctl replace 替代 istioctl create。

用浏览器再次打开 productpage (http://$GATEWAY_URL/productpage)，这次会看到 RBAC: access denied。Istio 的鉴权行为是“缺省拒绝”的，也就是说必须要显式的进行授权，才能对服务进行访问。

缓存和传播可能会造成一定的延迟。

### 命名空间级的访问控制
Istio 的授权能力可以轻松的设置命名空间级的访问控制，只要指定命名空间内的所有（或者部分）服务可以被另一命名空间的服务访问即可。

Bookinfo 示例中，productpage、reviews、details 以及 ratings 服务被部署在 default 命名空间中，而 istio-ingressgateway 等 Istio 组件是部署在 istio-system 命名空间中的。我们可以定义一个策略，default 命名空间中所有服务，如果其 app 标签取值在 productpage、reviews、details 以及 ratings 范围之内，就可以被本命名空间内以及 istio-system 命名空间内的服务进行访问。

运行这一命令，创建一个命名空间级别的访问控制策略：

$ istioctl create -f samples/bookinfo/platform/kube/rbac/namespace-policy.yaml

这一策略完成如下任务：

创建名为 service-viewer 的 ServiceRole，允许访问 default 命名空间中所有 app 标签值在 productpage、reviews、details 以及 ratings 范围之内的服务。注意其中的 constraint 字段，确定了服务的 app 标签取值必须在指定范围以内：
```
apiVersion: "rbac.istio.io/v1alpha1"
kind: ServiceRole
metadata:
  name: service-viewer
  namespace: default
spec:
  rules:
  - services: ["*"]
    methods: ["GET"]
    constraints:
    - key: "destination.labels[app]"
      values: ["productpage", "details", "reviews", "ratings"]
```
创建 ServiceRoleBinding 对象，用来把 service-viewer 角色指派给所有 istio-system 和 default 命名空间的服务：
```
apiVersion: "rbac.istio.io/v1alpha1"
kind: ServiceRoleBinding
metadata:
  name: bind-service-viewer
  namespace: default
spec:
  subjects:
  - properties:
      source.namespace: "istio-system"
  - properties:
      source.namespace: "default"
    roleRef:
    kind: ServiceRole
    name: "service-viewer"
```
命令执行结果大致如下：
```
servicerole "service-viewer" created
servicerolebinding "bind-service-viewer" created
```
如果这时用浏览器浏览 Bookinfo 的 productpage 页面 (http://$GATEWAY_URL/productpage)，会再次看到完整的页面，包含了左下角的 “Book Details” 以及右下角的 “Book Reviews”。

缓存和传播可能会造成一定的延迟。

清除命名空间级的访问控制
在进行后续任务之前，首先移除下面的配置：
```
$ istioctl delete -f samples/bookinfo/platform/kube/rbac/namespace-policy.yaml
```
### 服务级的访问控制
这个任务展示了使用 Istio 授权功能配置服务级访问控制的方法。开始之前，请进行下面的确认：

### 已经启用 Istio 授权
已经[清除命名空间级的访问控制](清除命名空间级的访问控制：
浏览器打开 Bookinfo 的 productpage (http://$GATEWAY_URL/productpage)。会看到：RBAC: access denied。我们会在 Bookinfo 中逐步为服务加入访问许可。

### 第一步，允许到 productpage 服务的访问
这里我们要创建一条策略，允许外部请求通过 Ingress 浏览 productpage。

执行命令：

$ istioctl create -f samples/bookinfo/platform/kube/rbac/productpage-policy.yaml

这条策略完成以下工作：

创建 ServiceRole，命名为 productpage-viewer，允许到 productpage 服务的读取访问：
```
apiVersion: "rbac.istio.io/v1alpha1"
kind: ServiceRole
metadata:
  name: productpage-viewer
  namespace: default
spec:
  rules:
  - services: ["productpage.default.svc.cluster.local"]
    methods: ["GET"]
```
创建 ServiceRole，并命名为 productpage-viewer，将 productpage-viewer 角色赋予给所有用户和服务：
```
apiVersion: "rbac.istio.io/v1alpha1"
kind: ServiceRoleBinding
metadata:
  name: bind-productpager-viewer
  namespace: default
spec:
  subjects:
  - user: "*"
    roleRef:
    kind: ServiceRole
    name: "productpage-viewer"
```
再次浏览 Bookinfo 的 productpage (http://$GATEWAY_URL/productpage)。应该能看到 “Bookinfo Sample” 页面了。但是还会显示 Error fetching product details 和 Error fetching product reviews 的错误信息。这是因为我们还没有给 productpage 访问 details 和 reviews 服务的授权。我们接下来就修复这个问题。

缓存和传播可能会造成一定的延迟。

### 第二步，允许对 details 和 reviews 服务的访问。
创建一条策略，让 productpage 服务能够读取 details 和 reviews 服务。注意在开始之前中，我们给 productpage 服务创建了一个命名为 bookinfo-productpage 的 Service account，它就是 productpage 服务的认证 ID。

运行下面的命令：
```
$ istioctl create -f samples/bookinfo/platform/kube/rbac/details-reviews-policy.yaml
```
这一策略完成以下任务：

- 创建一个 ServiceRole，命名为 details-reviews-viewer，允许对 details 和 reviews 服务进行只读访问。
```
apiVersion: "rbac.istio.io/v1alpha1"
kind: ServiceRole
metadata:
  name: details-reviews-viewer
  namespace: default
spec:
  rules:
  - services: ["details.default.svc.cluster.local", "reviews.default.svc.cluster.local"]
    methods: ["GET"]
```
- 创建一个 ServiceRoleBinding 并命名为 bind-details-review，用来把 details-reviews-viewer 角色授予给 cluster.local/ns/default/sa/bookinfo-productpage（也就是 productpage 服务的 Service account）。
```
apiVersion: "rbac.istio.io/v1alpha1"
kind: ServiceRoleBinding
metadata:
  name: bind-details-reviews
  namespace: default
spec:
  subjects:
  - user: "spiffe://cluster.local/ns/default/sa/bookinfo-productpage"
    roleRef:
    kind: ServiceRole
    name: "details-reviews-viewer"
```
浏览 Bookinfo 页面 productpage (http://$GATEWAY_URL/productpage)。现在看到的 “Bookinfo Sample” 中包含了左下角的 “Book Details” 以及右下角的 “Book Reviews”。然而 “Book Reviews” 中有一条错误信息： Ratings service currently unavailable，这是因为 reviews 服务无权访问 ratings 服务，要更正这一问题，就应该给 ratings 服务授权，使其能够访问 reviews 服务。下面的步骤就会完成这一需要。

缓存和传播可能会造成一定的延迟。

### 第三步，允许对 ratings 服务的访问
接下来新建一条策略，允许 reviews 服务对 ratings 发起读取访问。注意，我们在开始之前步骤里为 reviews 服务创建了 Service account bookinfo-reviews，这个账号就是 reviews 服务的认证凭据。

下面的命令会创建一条允许 reviews 服务读取 ratings 服务的策略。
```
$ istioctl create -f samples/bookinfo/platform/kube/rbac/ratings-policy.yaml
```
这条策略完成以下工作：

创建 ServiceRole 命名为 ratings-viewer，这一角色允许对 ratings 服务的访问。
```
apiVersion: "rbac.istio.io/v1alpha1"
kind: ServiceRole
metadata:
  name: ratings-viewer
  namespace: default
spec:
  rules:
  - services: ["ratings.default.svc.cluster.local"]
    methods: ["GET"]
```
创建 ServiceRoleBinding，命名为 bind-ratings，将 ratings-viewer 角色指派给 cluster.local/ns/default/sa/bookinfo-reviews，给这个 Service account 授权，也就就代表了给 reviews 服务授权。
```
apiVersion: "rbac.istio.io/v1alpha1"
kind: ServiceRoleBinding
metadata:
  name: bind-ratings
  namespace: default
spec:
  subjects:
  - user: "spiffe://cluster.local/ns/default/sa/bookinfo-reviews"
    roleRef:
    kind: ServiceRole
    name: "ratings-viewer"
```
用浏览器浏览 Bookinfo 应用的 productpage (http://$GATEWAY_URL/productpage)，应该就会看到 “Book Reviews” 区域中显示红色或者黑色的评级信息。

缓存和传播可能会造成一定的延迟。

### 清理
- 清理 Istio 授权策略的相关配置：
```
$ istioctl delete -f samples/bookinfo/platform/kube/rbac/ratings-policy.yaml
$ istioctl delete -f samples/bookinfo/platform/kube/rbac/details-reviews-policy.yaml
$ istioctl delete -f samples/bookinfo/platform/kube/rbac/productpage-policy.yaml
```
或者也可以运行命令删除所有的 ServiceRole 以及 ServiceRoleBinding 资源：
```
$ kubectl delete servicerole --all
$ kubectl delete servicerolebinding --all
```
- 禁用 Istio 的授权功能：
```
$ istioctl delete -f samples/bookinfo/platform/kube/rbac/rbac-config-ON.yaml
```

___

# 插入外部 CA 密钥和证书
开始之前
插入现有密钥和证书
检查新证书
清理

本任务展示运维人员如何使用现有根证书配置 Citadel 进行证书以及密钥的签发。

缺省情况下 Citadel 生成自签署的根证书和密钥，用于给工作负载签署证书。Citadel 还可以使用运维人员指定的根证书、证书和密钥进行工作负载的证书颁发。该任务所演示了向 Citadel 插入外部证书和密钥的方法。

### 开始之前
根据 quick start 内容，安装 Istio 并启用双向 TLS：
```
$ kubectl apply -f install/kubernetes/istio-demo-auth.yaml
```
或者

使用 Helm 并设置 global.mtls.enabled 为 true.

从 Istio 0.7 开始，可以使用认证策略来给命名空间中全部/部分服务配置双向 TLS 功能。（在所有命名空间中重复此操作，就相当于全局配置了）。这部分内容可参考认证策略任务
### 插入现有密钥和证书
假设我们想让 Citadel 使用现有的 ca-cert.pem 证书和 ca-key.pem，其中 ca-cert.pem 是由 root-cert.pem 根证书签发的，我们也准备使用 root-cert.pem 作为 Istio 工作负载的根证书。

下面的例子中，Citadel 的签署（CA）证书（root-cert.pem）不同于根证书（root-cert.pem），因此工作负载无法使用根证书进行证书校验。工作负载需要一个 cert-chain.pem 文件作为信任链，其中需要包含所有从根证书到工作负载证书之间的中间 CA。在我们的例子中，他包含了 Citadel 的签署证书，所以 cert-chain.pem 和 ca-cert.pem 是一致的。注意如果你的 ca-cert.pem 和 ca-cert.pem 是一致的，那么 cert-chain.pem 就是个空文件了。

这些文件都会在 samples/certs/ 目录中准备就绪提供使用。

下面的步骤在 Citadel 中插入了证书和密钥：

创建一个名为 cacert 的 secret，其中包含所有输入文件 ca-cert.pem、ca-key.pem、root-cert.pem 以及 cert-chain.pem：
```
$ kubectl create secret generic cacerts -n istio-system --from-file=samples/certs/ca-cert.pem \
    --from-file=samples/certs/ca-key.pem --from-file=samples/certs/root-cert.pem \
    --from-file=samples/certs/cert-chain.pem
```
重新部署 Citadel，它会从加载的 secret 中读取证书和密钥。
```
$ kubectl apply -f install/kubernetes/istio-citadel-plugin-certs.yaml
```
注意：如果使用不同的证书/密钥文件，或者不同的 secret 名称，需要根据实际情况变更 istio-citadel-plugin-certs.yaml

为了确定工作负载获取了正确的证书，删除 Citadel 生成的 Secret（命名为 istio.\*）。在本例中就是 istio.default。Citadel 会签发新的证书给工作负载。

```
$ kubectl delete secret istio.default
```

### 检查新证书
本节中，我们要校验新的工作负载证书以及根证书是否正确传播。需要在本机安装 openssl。

- 1.根据部署文档安装 Bookinfo 应用。

- 2.获取已加载的证书。

下面我们使用 ratings pod 作为例子，检查这个 Pod 上加载的证书。

用变量 RATINGSPOD 保存 Pod 名称：
```
$ RATINGSPOD=`kubectl get pods -l app=ratings -o jsonpath='{.items[0].metadata.name}'`
```
运行下列命令，获取 proxy 容器中加载的证书：
```
$ kubectl exec -it $RATINGSPOD -c istio-proxy -- /bin/cat /etc/certs/root-cert.pem > /tmp/pod-root-cert.pem
```
/tmp/pod-root-cert.pem 文件中包含落地到 Pod 中的根证书。
```
$ kubectl exec -it $RATINGSPOD -c istio-proxy -- /bin/cat /etc/certs/cert-chain.pem > /tmp/pod-cert-chain.pem
```
而 /tmp/pod-cert-chain.pem 这个文件则包含了工作负载证书以及传播到 Pod 中的 CA 证书

- 3.检查根证书和运维人员指定的证书是否一致：
```
$ openssl x509 -in samples/certs/root-cert.pem -text -noout > /tmp/root-cert.crt.txt
$ openssl x509 -in /tmp/pod-root-cert.pem -text -noout > /tmp/pod-root-cert.crt.txt
$ diff /tmp/root-cert.crt.txt /tmp/pod-root-cert.crt.txt
```
输出为空代表符合预期。

- 4.检查 CA 证书和运维人员指定的是否一致
```
$ tail -n 22 /tmp/pod-cert-chain.pem > /tmp/pod-cert-chain-ca.pem
$ openssl x509 -in samples/certs/ca-cert.pem -text -noout > /tmp/ca-cert.crt.txt
$ openssl x509 -in /tmp/pod-cert-chain-ca.pem -text -noout > /tmp/pod-cert-chain-ca.crt.txt
$ diff /tmp/ca-cert.crt.txt /tmp/pod-cert-chain-ca.crt.txt
```
输出为空代表符合预期。

- 5.检查从根证书到工作负载证书的证书链：
```
$ head -n 21 /tmp/pod-cert-chain.pem > /tmp/pod-cert-chain-workload.pem
$ openssl verify -CAfile <(cat samples/certs/ca-cert.pem samples/certs/root-cert.pem) /tmp/pod-cert-chain-workload.pem
/tmp/pod-cert-chain-workload.pem: OK
```
### 清理
- 移除 secret cacerts:
```
$ kubectl delete secret cacerts -n istio-system
```
- 移除 Istio 组件:
```
$ kubectl delete -f install/kubernetes/istio-demo-auth.yaml
```
___

Citadel 的健康检查
开始之前
部署启用健康检查的 Citadel
确认健康检查器的是否工作
(可选) 健康检查的配置
清理
See also
本文中的任务展示了如何在 Kubernetes 中为 Citadel 启动健康检查，注意，这一功能仍处于 Alpha 阶段。

从 Istio 0.6 开始，Citadel 具备了一个可选的健康检查功能。缺省情况下的 Istio 部署过程没有启用这一特性。目前健康检查功能通过周期性的向 API 发送 CSR 的方式，来检测 Citadel CSR 签署服务的故障。很快会实现更多的健康检查方法。

Citadel 包含了一个检测器模块，它会周期性的检查 Citadel 的状态（目前只是 gRPC 服务器的健康情况）。如果 Citadel 是健康的，检测器客户端会更新健康状态文件（文件内容始终为空）的更新时间。否则就什么都不做。Citadel 依赖 Kubernetes 的健康和就绪检测功能，会周期性的使用命令行检查健康状态文件的更新时间。如果这个文件有一段时间不更新了，Citadel 容器就会被 Kubelet 的重新启动。

注意：Citadel 的健康检查目前只提供了对 CSR 服务 API 的支持，如果没有使用 Istio Mesh Expansion （这个特性需要 CSR 服务接口的支持）就没有必要使用这个功能了。

### 开始之前
根据快速开始的指引部署 Istio 并启用全局双向 TLS 支持。
```
$ kubectl apply -f install/kubernetes/istio-demo-auth.yaml
```
或者

用 Helm 进行部署，设置 global.mtls.enabled 为 true。

Istio 0.7 开始，可以使用认证策略为命名空间内的部分或者全部服务配置双向 TLS 支持（在所有命名空间重复一遍就算是全局配置了）。请参考认证策略任务
部署启用健康检查的 Citadel
下面的命令用来部署启用健康检查的 Citadel：
```
$ kubectl apply -f install/kubernetes/istio-citadel-with-health-check.yaml
```
### 部署 istio-citadel 服务，这样健康检查器才能找到 CSR 服务.
```
cat <<EOF | kubectl create -f -
apiVersion: v1
kind: Service
metadata:
  name: istio-citadel
  namespace: istio-system
  labels:
    istio: citadel
spec:
  ports:
    - port: 8060
  selector:
    istio: citadel
EOF
```
### 确认健康检查器的是否工作
Citadel 会记录健康检查的结果，运行下面的命令行：
```
$ kubectl logs `kubectl get po -n istio-system | grep istio-citadel | awk '{print $1}'` -n istio-system
```
会看到类似下面这样的输出：
```
...
2018-02-27T04:29:56.128081Z     info    CSR successfully signed.
...
2018-02-27T04:30:11.081791Z     info    CSR successfully signed.
...
2018-02-27T04:30:25.485315Z     info    CSR successfully signed.
...
```
上面的日志表明周期性的健康检查已经启动。可以看到，缺省的健康检查的时间周期是 15 秒。

### (可选) 健康检查的配置
还可以根据需要调整健康检查的配置。打开文件 install/kubernetes/istio-citadel-with-health-check.yaml，找到下面的内容（注释已汉化，非原文）：
```
...
  - --liveness-probe-path=/tmp/ca.liveness # 健康检查状态文件的路径
  - --liveness-probe-interval=60s # 健康状态文件的更新周期
  - --probe-check-interval=15s    # 健康检查的周期
  - --logtostderr
  - --stderrthreshold
  - INFO
livenessProbe:
  exec:
    command:
    - /usr/local/bin/istio_ca
    - probe
    - --probe-path=/tmp/ca.liveness # 健康状态文件的路径
    - --interval=125s               # 文件修改时间和当前系统时钟的最大时间差
  initialDelaySeconds: 60
  periodSeconds: 60
...
```
- liveness-probe-path 和 probe-path：到健康状态文件的路径，在 Citadel 以及检测器上进行配置；
- liveness-probe-interval：是更新健康状态文件的周期；
- probe-check-interval：是 Citadel 健康检查的周期；
- interval：从上次更新健康状态文件至今的时间，也就是检测器认为 Citadel 健康的时间段；
- initialDelaySeconds 以及 periodSeconds：初始化延迟以及检测运行周期；
延长 probe-check-interval 会减少健康检查的开销，但是一旦遇到故障情况，健康监测器也会更晚的得到故障信息。为了避免检测器因为临时故障重启 Citadel，检测器的 interval 应该设置为 liveness-probe-interval 的 N 倍，这样就让检测器能够容忍持续 N-1 次的检查失败。

### 清理
- 在 Citadel 上禁用健康检查：
```
$ kubectl apply -f install/kubernetes/istio-demo-auth.yaml
$ kubectl delete svc istio-citadel -n istio-system
```
- 移除 Citadel：
```
$ kubectl delete -f install/kubernetes/istio-citadel-with-health-check.yaml
$ kubectl delete svc istio-citadel -n istio-system
```

___


# 双向 TLS 的迁移
### 开始之前
配置服务器使其同时能接收双向 TLS 以及明文流量
配置客户端进行双向 TLS 通信
锁定使用双向 TLS (可选)
清理
See also
本文任务展示了如何在不中断通信的情况下，把现存 Istio 服务的流量从明文升级为双向 TLS

在实际情况中，集群中可能包含 Istio 服务（注入了 Envoy sidecar）以及非 Istio 服务（没有注入 Envoy sidecar 的服务，下文简称为存量服务）。存量服务无法使用 Istio 签发的密钥/证书来进行双向 TLS 通信。我们希望安全的、渐进的启用双向 TLS。

### 开始之前
理解 Istio 认证策略以及相关的双向 TLS 认证概念。

已成功在 Kubernetes 集群中部署 Istio，并且没有启用双向 TLS 支持（也就是使用安装步骤中所说的 install/kubernetes/istio-demo.yaml 进行部署，或者在 Helm 安装时设置 global.mtls.enabled 的值为 false）。

为了演示目的，创建三个命名空间，分别是 foo、bar 以及 legacy，然后在 foo、bar 中分别部署注入 Istio sidecar 的 httpbin 以及 sleep 应用，最后在 legacy 命名空间中运行未经注入的 sleep 应用。
```
$ kubectl create ns foo
$ kubectl apply -f <(istioctl kube-inject -f samples/httpbin/httpbin.yaml) -n foo
$ kubectl apply -f <(istioctl kube-inject -f samples/sleep/sleep.yaml) -n foo
$ kubectl create ns bar
$ kubectl apply -f <(istioctl kube-inject -f samples/httpbin/httpbin.yaml) -n bar
$ kubectl apply -f <(istioctl kube-inject -f samples/sleep/sleep.yaml) -n bar
$ kubectl create ns legacy
$ kubectl apply -f samples/sleep/sleep.yaml -n legacy
```
检查部署情况：从任意一个命名空间选一个 sleep pod，发送 http 请求到 httpbin.foo。所有的请求都应该能返回 HTTP 200。
```
$ for from in "foo" "bar" "legacy"; do kubectl exec $(kubectl get pod -l app=sleep -n ${from} -o jsonpath={.items..metadata.name}) -c sleep -n ${from} -- curl http://httpbin.foo:8000/ip -s -o /dev/null -w "sleep.${from} to httpbin.foo: %{http_code}\n"; done
sleep.foo to httpbin.foo: 200
sleep.bar to httpbin.foo: 200
sleep.legacy to httpbin.foo: 200
```
确认系统中不存在认证策略和目标规则：
```
$ kubectl get policies.authentication.istio.io --all-namespaces
No resources found.
$ kubectl get destionationrule --all-namespaces
No resources found.
```
### 配置服务器使其同时能接收双向 TLS 以及明文流量
在认证策略中有一个 PERMISSIVE 模式，这种模式让服务器能够同时接收明文和双向 TLS 流量。下面就把服务器设置为这种模式：
```
cat <<EOF | istioctl create -n foo -f -
apiVersion: "authentication.istio.io/v1alpha1"
kind: "Policy"
metadata:
  name: "example-httpbin-permissive"
  namespace: foo
spec:
  targets:
  - name: httpbin
    peers:
  - mtls:
      mode: PERMISSIVE
EOF
```
接下来再次发送流量到 httpbin.foo，确认所有请求依旧成功。
```
$ for from in "foo" "bar" "legacy"; do kubectl exec $(kubectl get pod -l app=sleep -n ${from} -o jsonpath={.items..metadata.name}) -c sleep -n ${from} -- curl http://httpbin.foo:8000/ip -s -o /dev/null -w "sleep.${from} to httpbin.foo: %{http_code}\n"; done
200
200
200
```
### 配置客户端进行双向 TLS 通信
利用设置 DestinationRule 的方式，让 Istio 服务进行双向 TLS 通信。
```
cat <<EOF | istioctl create -n foo -f -
apiVersion: "networking.istio.io/v1alpha3"
kind: "DestinationRule"
metadata:
  name: "example-httpbin-istio-client-mtls"
spec:
  host: httpbin.foo.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
EOF
```
这样一来，sleep.foo 和 sleep.bar 就会开始使用双向 TLS 和 httpbin.foo 进行通信了。而 sleep.legacy 因为没有进行 sidecar 注入，因此不受 DestinationRule 配置影响，还是会使用明文和 httpbin.foo 通信。

现在复查一下，所有到 httpbin.foo 的通信是否依旧成功：
```
$ for from in "foo" "bar" "legacy"; do kubectl exec $(kubectl get pod -l app=sleep -n ${from} -o jsonpath={.items..metadata.name}) -c sleep -n ${from} -- curl http://httpbin.foo:8000/ip -s -o /dev/null -w "sleep.${from} to httpbin.foo: %{http_code}\n"; done
200
200
200
```
还可以在 DestinationRule 中指定一个客户端的子集所发出的请求来是用双向 TLS 通信，然后使用 Grafana 验证配置执行情况，确认通过之后，将策略的应用范围扩大到该服务的所有子集。

### 锁定使用双向 TLS (可选)
把所有进行过 sidecar 注入的客户端到服务器流量都迁移到双向 TLS 之后，就可以设置 httpbin.foo 只支持双向 TLS 流量了。
```
cat <<EOF | istioctl create -n foo -f -
apiVersion: "authentication.istio.io/v1alpha1"
kind: "Policy"
metadata:
  name: "example-httpbin-permissive"
  namespace: foo
spec:
  targets:
  - name: httpbin
    peers:
  - mtls:
      mode: STRICT
EOF
```
这样设置之后，sleep.legacy 的请求就会失败。
```
$ for from in "foo" "bar" "legacy"; do kubectl exec $(kubectl get pod -l app=sleep -n ${from} -o jsonpath={.items..metadata.name}) -c sleep -n ${from} -- curl http://httpbin.foo:8000/ip -s -o /dev/null -w "sleep.${from} to httpbin.foo: %{http_code}\n"; done
200
200
503
```
也就是说，如果不能把所有服务都迁移到 Istio (进行 Sidecar 注入)的话，就只能使用 PERMISSIVE 模式了。然而在配置为 PERMISSIVE 的时候，是不会对明文流量进行授权和鉴权方面的检查的。我们推荐使用 RBAC 来给不同的路径配置不同的授权策略。

### 清理
移除所有资源
```
$ kubectl delete ns foo bar legacy
Namespaces foo bar legacy deleted.
```

___


# 通过 HTTPS 进行双向 TLS
开始之前
生成证书和 configmap
在没有 Istio sidecar 的情况下部署 HTTPS 服务
使用 Istio sidecar 和禁用双向 TLS 创建 HTTPS 服务
用 Istio sidecar 创建一个 HTTPS 服务，并使用双向 TLS
清除
See also
这个任务展示了 Istio 双向 TLS 是如何与 HTTPS 服务一起工作的。它包括:

在没有 Istio sidecar 的情况下部署 HTTPS 服务
关闭 Istio 双向 TLS 认证情况下部署 HTTPS 服务
部署一个启动双向 TLS 的 HTTPS 服务。对于每个部署，请连接到此服务并验证其是否有效。
当 Istio sidecar 使用 HTTPS 服务部署时，代理将自动从 L7 降至 L4（无论是否启用了双向 TLS），这就意味着它不会终止原来的 HTTPS 通信。这就是为什么 Istio 可以在 HTTPS 服务上工作。

### 开始之前
按照下面的快速开始设置 Istio。注意，在安装步骤第5步中，身份验证应该被禁用。

### 生成证书和 configmap
您需要安装 openssl 来运行以下命令：
```
$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/nginx.key -out /tmp/nginx.crt -subj "/CN=my-nginx/O=my-nginx"
$ kubectl create secret tls nginxsecret --key /tmp/nginx.key --cert /tmp/nginx.crt
secret "nginxsecret" created
```
### 创建用于 HTTPS 服务的 configmap
```
$ kubectl create configmap nginxconfigmap --from-file=samples/https/default.conf
configmap "nginxconfigmap" created
```
### 在没有 Istio sidecar 的情况下部署 HTTPS 服务
本节将创建一个基于 nginx 的 HTTPS 服务。
```
$ kubectl apply -f samples/https/nginx-app.yaml
service "my-nginx" created
replicationcontroller "my-nginx" created
```
然后，创建另一个 pod 来调用这个服务。
```
$ kubectl apply -f <(bin/istioctl kube-inject -f samples/sleep/sleep.yaml)
```
获取 pods
```
$ kubectl get pod
NAME                              READY     STATUS    RESTARTS   AGE
my-nginx-jwwck                    1/1       Running   0          1h
sleep-847544bbfc-d27jg            2/2       Running   0          18h
```
SSH 进入包含 sleep pod 的 istio-proxy 容器。
```
$ kubectl exec -it $(kubectl get pod -l app=sleep -o jsonpath={.items..metadata.name}) -c istio-proxy /bin/bash
```
调用 my-nginx
```
$ curl https://my-nginx -k
...
<h1>Welcome to nginx!</h1>
...
```
你可以把上面的三个命令合并成一个：
```
$ kubectl exec $(kubectl get pod -l app=sleep -o jsonpath={.items..metadata.name}) -c istio-proxy -- curl https://my-nginx -k
...
<h1>Welcome to nginx!</h1>
...
```
### 使用 Istio sidecar 和禁用双向 TLS 创建 HTTPS 服务
在”开始之前”部分中，Istio 控制平面被部署在双向 TLS 禁用的情况下。所以您只需要使用 sidecar 重新部署 NGINX HTTPS 服务。

删除这个 HTTPS 服务
```
$ kubectl delete -f samples/https/nginx-app.yaml
```
用一个 sidecar 来部署它
```
$ kubectl apply -f <(bin/istioctl kube-inject -f samples/https/nginx-app.yaml)
```
确保这个 pod 已经启动并运行
```
$ kubectl get pod
NAME                              READY     STATUS    RESTARTS   AGE
my-nginx-6svcc                    2/2       Running   0          1h
sleep-847544bbfc-d27jg            2/2       Running   0          18h
```
运行
```
$ kubectl exec sleep-847544bbfc-d27jg -c sleep -- curl https://my-nginx -k
...
<h1>Welcome to nginx!</h1>
...
```
如果从 istio-proxy 容器运行，它也应该正常运行
```
$ kubectl exec $(kubectl get pod -l app=sleep -o jsonpath={.items..metadata.name}) -c istio-proxy -- curl https://my-nginx -k
...
<h1>Welcome to nginx!</h1>
...
```
这个例子是从 Kubernetes 的例子中引用的。

### 用 Istio sidecar 创建一个 HTTPS 服务，并使用双向 TLS
您需要使用启用了双向 TLS 的 Istio 控制平面。如果您已经安装了 istio 控制平面，并安装了双向 TLS，请删除它：
```
$ kubectl delete -f install/kubernetes/istio.yaml
```
等待一切都完成了，也就是说在控制平面名称空间（istio-system）中没有 pod。
```
$ kubectl get pod -n istio-system
No resources found.
```
然后，部署启用了双向 TLS 的 Istio 控制平面：
```
$ kubectl apply -f install/kubernetes/istio-auth.yaml
```
确保一切正常运转：
```
$ kubectl get po -n istio-system
NAME                                       READY     STATUS      RESTARTS   AGE
grafana-6f6dff9986-r6xnq                   1/1       Running     0          23h
istio-citadel-599f7cbd46-85mtq             1/1       Running     0          1h
istio-cleanup-old-ca-mcq94                 0/1       Completed   0          23h
istio-egressgateway-78dd788b6d-jfcq5       1/1       Running     0          23h
istio-ingressgateway-7dd84b68d6-dxf28      1/1       Running     0          23h
istio-mixer-post-install-g8n9d             0/1       Completed   0          23h
istio-pilot-d5bbc5c59-6lws4                2/2       Running     0          23h
istio-policy-64595c6fff-svs6v              2/2       Running     0          23h
istio-sidecar-injector-645c89bc64-h2dnx    1/1       Running     0          23h
istio-statsd-prom-bridge-949999c4c-mv8qt   1/1       Running     0          23h
istio-telemetry-cfb674b6c-rgdhb            2/2       Running     0          23h
istio-tracing-754cdfd695-wqwr4             1/1       Running     0          23h
prometheus-86cb6dd77c-ntw88                1/1       Running     0          23h
servicegraph-5849b7d696-jrk8h              1/1       Running     0          23h
```
然后重新部署 HTTPS 服务和 sleep 服务
```
$ kubectl delete -f <(bin/istioctl kube-inject -f samples/sleep/sleep.yaml)
$ kubectl apply -f <(bin/istioctl kube-inject -f samples/sleep/sleep.yaml)
$ kubectl delete -f <(bin/istioctl kube-inject -f samples/https/nginx-app.yaml)
$ kubectl apply -f <(bin/istioctl kube-inject -f samples/https/nginx-app.yaml)
```
确保 pod 已启动并正在运行
```
$ kubectl get pod
NAME                              READY     STATUS    RESTARTS   AGE
my-nginx-9dvet                    2/2       Running   0          1h
sleep-77f457bfdd-hdknx            2/2       Running   0          18h
```
运行
```
$ kubectl exec $(kubectl get pod -l app=sleep -o jsonpath={.items..metadata.name}) -c sleep -- curl https://my-nginx -k
...
<h1>Welcome to nginx!</h1>
...
```
因为工作流”sleep –> sleep-proxy –> nginx-proxy –> nginx”，整个过程是7层流量，在 sleep-proxy 和 nginx-proxy 之间有一个 L4 双向 TLS 加密。在这种情况下，一切都很好。

但是，如果您从 istio-proxy 容器运行这个命令，它将无法工作。
```
$ kubectl exec $(kubectl get pod -l app=sleep -o jsonpath={.items..metadata.name}) -c istio-proxy -- curl https://my-nginx -k
curl: (35) gnutls_handshake() failed: Handshake failed
command terminated with exit code 35
```
原因是对于工作流”sleep-proxy –> nginx-proxy –> nginx”，nginx-proxy 可以从 sleep-proxy 中获得双向的 TLS 流量。在上面的命令中，sleep-proxy 不提供客户端证书，因此它不会起作用。此外，即使是 sleep-proxy 可以在上面的命令中提供客户端证书，它也不会工作，因为流量会从 nginx-proxy 降级到 nginx。

### 清除
```
$ kubectl delete -f samples/sleep/sleep.yaml
$ kubectl delete -f samples/https/nginx-app.yaml
$ kubectl delete configmap nginxconfigmap
$ kubectl delete secret nginxsecret
```