# 10.流量管理
演示Istio流量路由功能的任务。

### 配置请求路由
此任务向您展示如何根据权重和 HTTP header配置动态请求路由。
### 开始之前
按照安装指南中的说明安装 Istio。
部署 Bookinfo 示例应用程序。

#### 基于内容的路由
由于 Bookinfo 示例部署了三个版本的 reviews 微服务，因此我们需要设置默认路由。 否则，如果您当多次访问应用程序，您会注意到有时输出包含星级评分，有时又没有。 这是因为没有为应用明确指定缺省路由时，Istio 会将请求随机路由到该服务的所有可用版本上。

此任务假定您尚未设置任何路由。 如果您已经为示例应用程序创建了存在冲突的路由规则，则需要在下面的命令中使用 replace 代替 create。 请注意：本文档假设还没有设置任何路由规则。如果

###### 1.将所有微服务的默认版本设置为 v1。
```
$ istioctl create -f samples/bookinfo/networking/destination-rule-all.yaml
```
如果您启用了 mTLS ，请运行以下代码
```
$ istioctl create -f samples/bookinfo/networking/destination-rule-all-mtls.yaml

$ istioctl create -f samples/bookinfo/networking/virtual-service-all-v1.yaml
```
在kubernetes中部署 Istio 时，您可以在上面及其它所有命令行中用 kubectl 代替 istioctl。 但请注意，目前 kubectl 不提供输入验证。

您可以通过下面的命令来显示已创建的路由规则:
```
$ istioctl get virtualservices -o yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: details
  ...
spec:
  hosts:
  - details
    http:
  - route:
    - destination:
        host: details
        subset: v1
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: productpage
  ...
spec:
  gateways:
  - bookinfo-gateway
  - mesh
    hosts:
  - productpage
    http:
  - route:
    - destination:
        host: productpage
        subset: v1
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: ratings
  ...
spec:
  hosts:
  - ratings
    http:
  - route:
    - destination:
        host: ratings
        subset: v1
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: reviews
  ...
spec:
  hosts:
  - reviews
    http:
  - route:
    - destination:
        host: reviews
        subset: v1
---
```
可以使用 istioctl get destinationrules -o yaml来显示路由规则对应的 subset 定义。

由于路由规则是通过异步方式分发到代理的，因此在尝试访问应用程序之前，您应该等待几秒钟，以便规则传播到所有 pod 上。

###### 2.在浏览器中打开 Bookinfo 应用程序的 URL (http://$GATEWAY_URL/productpage)。 回想一下，在部署 Bookinfo 示例时，应已参照该说明设置好 GATEWAY_URL 。

您应该可以看到 Bookinfo 应用程序的 productpage 页面。 请注意， productpage 页面显示的内容中没有评分星级，这是因为 reviews:v1 服务不会访问 ratings 服务。

###### 3.将来自特定用户的请求路由到 reviews:v2。

通过将来自 productpage 的流量路由到 reviews:v2 实例，为测试用户 “jason” 启用 ratings 服务。
```
$ istioctl replace -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml
```
确认规则已创建：
```
$ istioctl get virtualservice reviews -o yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: reviews
  ...
spec:
  hosts:
  - reviews
    http:
  - match:
    - headers:
        end-user:
          exact: jason
    route:
    - destination:
        host: reviews
        subset: v2
  - route:
    - destination:
        host: reviews
        subset: v1
```
###### 4.在 productpage 网页上以用户 “jason” 身份登录。

您现在应该在每次评论旁边看到评分（1-5颗星）。 请注意，如果您以任何其他用户身份登录，您将会继续看到 reviews:v1 版本服务，即不包含星级评价的页面。

### 理解原理
在此任务中，您首先使用 Istio 将 100% 的请求流量都路由到了 Bookinfo 服务的 v1 版本。 然后再设置了一条路由规则，该路由规则在 productpage 服务中添加基于请求的 “end-user” 自定义 header 选择性地将特定的流量路由到了 reviews 服务的 v2 版本。

请注意，为了利用 Istio 的 L7 路由功能，Kubernetes 中的服务（如本任务中使用的 Bookinfo 服务）必须遵守某些特定限制。 参考 sidecar 注入文档了解详情。

在流量转移任务中，您将按照在此处学习的相同基本模式来配置路由规则，以逐步将流量从服务的一个版本发送到另一个版本。

清除
删除应用程序 virtual service。

$ istioctl delete -f samples/bookinfo/networking/virtual-service-all-v1.yaml

如果您不打算探索任何后续任务，请参阅 Bookinfo 清理 的说明关闭应用程序。
___
### 故障注入
此任务说明如何注入延迟并测试应用程序的弹性。
### 前提条件
- 按照安装指南中的说明设置 Istio 。

- 部署示例应用程序 Bookinfo 。

- 通过首先执行请求路由任务或运行以下命令来初始化应用程序版本路由：
```
$ istioctl create -f samples/bookinfo/networking/virtual-service-all-v1.yaml
$ istioctl replace -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml
```
#### 使用 HTTP 延迟进行故障注入
为了测试我们的微服务应用程序 Bookinfo 的弹性，我们将在 reviews :v2 和 ratings 服务之间的一个用户 “jason” 注入一个 7 秒 的延迟。 由于 reviews:v2 服务对其 ratings 服务的调用具有 10 秒的硬编码连接超时，因此我们期望端到端流程是正常的（没有任何错误）。

创建故障注入规则以延迟来自用户 “jason”（我们的测试用户）的流量
```
$ istioctl replace -f samples/bookinfo/networking/virtual-service-ratings-test-delay.yaml
```
确认已创建规则：
```
$ istioctl get virtualservice ratings -o yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: ratings
  ...
spec:
  hosts:
  - ratings
    http:
  - fault:
      delay:
        fixedDelay: 7s
        percent: 100
    match:
    - headers:
        end-user:
          exact: jason
    route:
    - destination:
        host: ratings
        subset: v1
  - route:
    - destination:
        host: ratings
        subset: v1
```
规则可能需要几秒钟才能传播到所有的 pod 。

观察应用程序行为

以 “jason” 用户身份登录。如果应用程序的首页设置为正确处理延迟，我们预计它将在大约 7 秒内加载。 要查看网页响应时间，请在IE，Chrome 或 Firefox 中打开 Developer Tools 菜单（通常，组合键 Ctrl+Shift+I 或 Alt+Cmd+I ）， 选项卡 Network，然后重新加载 productpage 网页 。

您将看到网页加载大约 6 秒钟。评论部分将显示 对不起，此书的产品评论目前不可用 。

### 了解发生了什么
整个评论服务失败的原因是我们的 Bookinfo 应用程序有错误。 产品页面和评论服务之间的超时（评分为 3 次+ 1 次重试 = 总共 6 次）比评论和评级服务之间的超时时间（硬编码连接超时为 10 秒）。 这些类型的错误可能发生在典型的企业应用程序中，其中不同的团队独立地开发不同的微服务。 Istio 的故障注入规则可帮助您识别此类异常，而不会影响最终用户。

请注意，我们仅限制用户 “jason” 的失败影响。, 如果您以任何其他用户身份登录，则不会遇到任何延迟。

修复错误： 此时我们通常会通过增加产品页面超时或减少评级服务超时的评论来解决问题， 终止并重启固定的微服务，然后确认 productpage 返回其响应, 没有任何错误。

但是，我们已经在评论服务的第 3 版中运行此修复程序， 因此我们可以通过将所有流量迁移到 reviews:v3 来解决问题， 如流量转移中所述任务。

（作为读者的练习 - 将延迟规则更改为使用 2.8 秒延迟，然后针对 v3 版本的评论运行它。）

### 使用 HTTP Abort 进行故障注入
作为弹性的另一个测试，我们将在 ratings 服务中，给用户 jason 的调用加上一个 HTTP 中断 。 我们希望页面能够立即加载，而不像延迟示例那样显示”产品评级不可用”消息。

1.为用户 “jason” 创建故障注入规则发送 HTTP 中止
```
$ istioctl replace -f samples/bookinfo/networking/virtual-service-ratings-test-abort.yaml
```
确认已创建规则
```
$ istioctl get virtualservice ratings -o yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: ratings
  ...
spec:
  hosts:
  - ratings
    http:
  - fault:
      abort:
        httpStatus: 500
        percent: 100
    match:
    - headers:
        end-user:
          exact: jason
    route:
    - destination:
        host: ratings
        subset: v1
  - route:
    - destination:
        host: ratings
        subset: v1
```
2.观察应用程序行为

以 “jason” 用户名登录, 如果规则成功传播到所有的 pod ，您应该能立即看到页面加载”产品评级不可用”消息。 从用户 “jason” 注销，您应该会在产品页面网页上看到评级星标的评论成功显示。

### 清理
删除应用程序路由规则：
```
$ istioctl delete -f samples/bookinfo/networking/virtual-service-all-v1.yaml
```
如果您不打算探索任何后续任务，请参阅 Bookinfo 清理说明以关闭应用程序。

____

### 流量转移
向您展示如何将流量从旧版本迁移到新版本的服务。
### 开始之前
- 按照安装指南中的说明安装Istio。
- 部署 Bookinfo 示例应用程序。
- 查看 流量管理 概念文档。
### 关于这个任务
一个常见的用例是将流量从一个版本的微服务逐渐迁移到另一个版本。 在Istio中，您可以通过配置一系列规则来实现此目标， 这些规则将一定百分比的流量路由到一个或另一个服务。 在此任务中，您将先分别向 reviews:v1 和 reviews:v3 各发送50%流量。 然后，您将通过向 reviews:v3 发送100％的流量来完成迁移。

### 应用基于权重的路由
1.首先，运行此命令将所有流量路由到 v1 版本的各个微服务。
```
$ istioctl create -f samples/bookinfo/networking/virtual-service-all-v1.yaml
```
2.在浏览器中打开 Bookinfo 站点。 URL为 http://$GATEWAY_URL/productpage，其中 $GATEWAY_URL是 ingress 的外部IP地址， 其描述参见 Bookinfo。

请注意，不管刷新多少次，页面的评论部分都不会显示评级星号。这是因为 Istio 被配置为将 reviews 服务的的所有流量都路由到了 reviews：v1 版本， 而该版本的服务不会访问带星级的 ratings 服务。

3.使用下面的命令把50%的流量从 reviews:v1 转移到 reviews:v3:
```
$ istioctl replace -f samples/bookinfo/networking/virtual-service-reviews-50-v3.yaml
```
等待几秒钟以让新的规则传播到代理中生效。

4.确认规则已被替换:
```
$ istioctl get virtualservice reviews -o yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: reviews
  ...
spec:
  hosts:
  - reviews
  http:
  - route:
    - destination:
        host: reviews
        subset: v1
      weight: 50
    - destination:
        host: reviews
        subset: v3
      weight: 50
```
5.刷新浏览器中的 /productpage 页面，大约有50%的几率会看到页面中出带红色星级的评价内容。这是因为 v3 版本的 reviews 访问了带星级评级的 ratings 服务，但v1版本却没有。

在目前的Envoy sidecar实现中，可能需要刷新 /productpage 很多次–可能15次或更多–才能看到流量分发的效果。您可以通过修改规则将90%的流量路由到v3，这样看到更多带红色星级的评价。

6.如果您认为 reviews：v3 微服务已经稳定，你可以通过应用此 virtual service 将100％的流量路由到 reviews：v3：
```
$ istioctl replace -f samples/bookinfo/networking/virtual-service-reviews-v3.yaml
```
现在，当您刷新 /productpage 时，您将始终看到带有红色星级评分的书评。

### 了解发生了什么
在这项任务中，我们使用Istio的加权路由功能将流量从旧版本的 reviews 服务迁移到新版本。请注意，这和使用容器编排平台的部署功能来进行版本迁移完全不同，后者使用了实例扩容来对流量进行管理。

使用Istio，两个版本的 reviews 服务可以独立地进行扩容和缩容，并不会影响这两个版本服务之间的流量分发。

如果想了解支持自动伸缩的版本路由的更多信息，请查看使用 Istio 的 Canary Deployments 。

### 清理
删除应用程序路由规则。
```
$ istioctl delete -f samples/bookinfo/networking/virtual-service-all-v1.yaml
```
如果您不打算探索任何后续任务，请参阅 Bookinfo 清理 的说明关闭应用程序。
___
### 设置请求超时
本任务用于示范如何使用 Istio 在 Envoy 中设置请求超时。
### 开始之前
- 跟随安装指南设置 Istio。

- 部署示例应用程序 Bookinfo 。

- 使用下面的命令初始化应用的版本路由：
```
$ istioctl create -f samples/bookinfo/networking/virtual-service-all-v1.yaml
```
### 请求超时
可以在路由规则的 timeout 字段中来给 http 请求设置请求超时。缺省情况下，超时被设置为 15 秒钟，本文任务中，会把 reviews 服务的超时设置为一秒钟。为了能观察设置的效果，还需要在对 ratings 服务的调用中加入两秒钟的延迟。

1.到 reviews:v2 服务的路由定义：
```
cat <<EOF | istioctl replace -f -
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: reviews
spec:
  hosts:
    - reviews
  http:
  - route:
    - destination:
        host: reviews
        subset: v2
EOF
```
2.在对 ratings 服务的调用中加入两秒钟的延迟：
```
cat <<EOF | istioctl replace -f -
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: ratings
spec:
  hosts:
  - ratings
  http:
  - fault:
      delay:
        percent: 100
        fixedDelay: 2s
    route:
    - destination:
        host: ratings
        subset: v1
EOF
```
3.用浏览器打开网址 http://$GATEWAY_URL/productpage，浏览 Bookinfo 应用。

这时应该能看到 Bookinfo 应用在正常运行（显示了评级的星形符号），但是每次刷新页面，都会出现两秒钟的延迟。

4.接下来在目的为 reviews:v2 服务的请求加入一秒钟的请求超时：
```
cat <<EOF | istioctl replace -f -
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: reviews
spec:
  hosts:
  - reviews
  http:
  - route:
    - destination:
        host: reviews
        subset: v2
    timeout: 1s
EOF
```
5.刷新 Bookinfo 的 Web 页面。

这时候应该看到一秒钟就会返回，而不是之前的两秒钟，但 reviews 的显示已经不见了。

### 发生了什么？
上面的任务中，使用 Istio 为调用 reviews 微服务的请求中加入了一秒钟的超时控制，覆盖了缺省的 15 秒钟设置。页面刷新时，reviews 服务后面会调用 ratings 服务，使用 Istio 在对 ratings 的调用中注入了两秒钟的延迟，这样就让 reviews 服务要花费超过一秒钟的时间来调用 ratings 服务，从而触发了我们加入的超时控制。

这样就会看到 Bookinfo 的页面（ 页面由 reviews 服务生成）上没有出现 reviews 服务的显示内容，取而代之的是错误信息：Sorry, product reviews are currently unavailable for this book ，出现这一信息的原因就是因为来自 reviews 服务的超时错误。

如果测试了故障注入任务，会发现 productpage 微服务在调用 reviews 微服务时，还有自己的应用级超时设置（三秒钟）。注意这里我们用路由规则设置了一秒钟的超时。如果把超时设置为超过三秒钟（例如四秒钟）会毫无效果，这是因为内部的服务中设置了更为严格的超时要求。更多细节可以参见故障处理 FAQ 的相关内容。

还有一点关于 Istio 中超时控制方面的补充说明，除了像本文一样在路由规则中进行超时设置之外，还可以进行请求一级的设置，只需在应用的外发流量中加入 x-envoy-upstream-rq-timeout-ms Header 即可。在这个 Header 中的超时设置单位是毫秒而不是秒。

### 清理
移除应用的路由规则：
```
$ istioctl delete -f samples/bookinfo/networking/virtual-service-all-v1.yaml
```
如果不准备继续探索后续任务，根据 Bookinfo 清理内容来关停示例应用。

### 控制 Ingress 流量
介绍在服务网格 Istio 中如何配置外部公开服务。

前提条件
- 按照安装指南中的说明设置 Istio 。

- 确保您当前的目录是 istio 目录。

- 启动 httpbin 样本，该样本将用作要在外部公开的目标服务。

如果您已启用自动注入 Sidecar，请执行
```
$ kubectl apply -f samples/httpbin/httpbin.yaml
```
否则就必须在部署 httpbin 应用程序之前手动注入 Sidecar：
```
$ kubectl apply -f <(istioctl kube-inject -f samples/httpbin/httpbin.yaml)
```
按照以下小节中的说明确定 Ingress IP 和端口。
### 确定入口 IP 和端口
执行以下命令以确定您的 Kubernetes 集群是否在支持外部负载均衡器的环境中运行。
```
$ kubectl get svc istio-ingressgateway -n istio-system
NAME                   TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)                                      AGE
istio-ingressgateway   LoadBalancer   172.21.109.129   130.211.10.121  80:31380/TCP,443:31390/TCP,31400:31400/TCP   17h
```
如果 EXTERNAL-IP 设置了该值，则要求您的环境具有可用于 Ingress 网关的外部负载均衡器。如果 EXTERNAL-IP 值是 <none>（或一直是 <pending> ），则说明可能您的环境不支持为 ingress 网关提供外部负载均衡器的功能。在这种情况下，您可以使用 Service 的 node port 方式访问网关。

### 使用外部负载均衡器时确定 IP 和端口
```
$ export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
$ export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http")].port}')
$ export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="https")].port}')
```
### 确定使用 Node Port 时的 ingress IP 和端口
确定端口：
```
$ export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http")].nodePort}')
$ export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="https")].nodePort}')
```
确定 ingress IP 的具体方法取决于集群提供商。

1.GKE：
```
$ export INGRESS_HOST=<workerNodeAddress>
```
您需要创建防火墙规则以允许 TCP 流量进入 ingress gateway 服务的端口。运行以下命令以允许 HTTP 端口，安全端口（HTTPS）或两者的流量。
```
$ gcloud compute firewall-rules create allow-gateway-http --allow tcp:$INGRESS_PORT
$ gcloud compute firewall-rules create allow-gateway-https --allow tcp:$SECURE_INGRESS_PORT
```
2.IBM Cloud Kubernetes 服务免费版本：
```
$ bx cs workers <cluster-name or id>
$ export INGRESS_HOST=<public IP of one of the worker nodes>
```
3.Minikube：
```
$ export INGRESS_HOST=$(minikube ip)
```
4.其他环境（例如IBM Cloud Private等）：
```
$ export INGRESS_HOST=$(kubectl get po -l istio=ingressgateway -n istio-system -o 'jsonpath={.items[0].status.hostIP}')
```
### 使用 Istio 网关配置 Ingress
Ingress Gateway描述了在网格边缘操作的负载平衡器，用于接收传入的 HTTP/TCP 连接。它配置暴露的端口，协议等，但与 Kubernetes Ingress Resources 不同，它不包括任何流量路由配置。流入流量的流量路由使用 Istio 路由规则进行配置，与内部服务请求完全相同。

让我们看看如何为 Gateway 在 HTTP 80 端口上配置流量。

1.创建一个 Istio Gateway：
```
cat <<EOF | istioctl create -f -
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: httpbin-gateway
spec:
  selector:
    istio: ingressgateway # use Istio default gateway implementation
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "httpbin.example.com"
EOF
```
2.为通过 Gateway 进入的流量配置路由：
```
cat <<EOF | istioctl create -f -
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: httpbin
spec:
  hosts:
  - "httpbin.example.com"
    gateways:
  - httpbin-gateway
    http:
  - match:
    - uri:
        prefix: /status
    - uri:
        prefix: /delay
    route:
    - destination:
        port:
          number: 8000
        host: httpbin
EOF
```
在这里，我们 为服务创建了一个虚拟服务配置 httpbin ，其中包含两条路由规则，允许路径 /status 和 路径的流量 /delay。

该网关列表指定，只有通过我们的要求 httpbin-gateway 是允许的。所有其他外部请求将被拒绝，并返回 404 响应。

请注意，在此配置中，来自网格中其他服务的内部请求不受这些规则约束，而是简单地默认为循环路由。要将这些（或其他规则）应用于内部调用，我们可以将特殊值 mesh 添加到 gateways 的列表中。

3.使用 curl 访问 httpbin 服务：
```
$ curl -I -HHost:httpbin.example.com http://$INGRESS_HOST:$INGRESS_PORT/status/200
HTTP/1.1 200 OK
server: envoy
date: Mon, 29 Jan 2018 04:45:49 GMT
content-type: text/html; charset=utf-8
access-control-allow-origin: *
access-control-allow-credentials: true
content-length: 0
x-envoy-upstream-service-time: 48
```
请注意，这里使用该 -H 标志将 Host HTTP Header 设置为 “httpbin.example.com”。这以操作是必需的，因为上面的 Ingress Gateway 被配置为处理 “httpbin.example.com”，但在测试环境中没有该主机的 DNS 绑定，只是将请求发送到 Ingress IP。

4.访问任何未明确公开的其他 URL，应该会看到一个 HTTP 404 错误：
```
$ curl -I -HHost:httpbin.example.com http://$INGRESS_HOST:$INGRESS_PORT/headers
HTTP/1.1 404 Not Found
date: Mon, 29 Jan 2018 04:45:49 GMT
server: envoy
content-length: 0
```
### 使用浏览器访问 Ingress 服务
在浏览器中输入 httpbin 服务的地址是不会生效的，这是因为因为我们没有办法让浏览器像 curl 一样装作访问 httpbin.example.com。而在现实世界中，因为有正常配置的主机和 DNS 记录，这种做法就能够成功了——只要简单的在浏览器中访问由域名构成的 URL 即可，例如 https://httpbin.example.com/status/200。

要解决此问题以进行简单的测试和演示，我们可以在 Gateway 和 VirutualService 配置中为主机使用通配符值 *。例如，如果我们将 Ingress 配置更改为以下内容：
```
cat <<EOF | istioctl replace -f -
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: httpbin-gateway
spec:
  selector:
    istio: ingressgateway # use Istio default gateway implementation
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "*"
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: httpbin
spec:
  hosts:
  - "*"
  gateways:
  - httpbin-gateway
    http:
  - match:
    - uri:
        prefix: /headers
    route:
    - destination:
        port:
          number: 8000
        host: httpbin
EOF
```
接下来就可以在浏览器的 URL 中使用 $INGRESS_HOST:$INGRESS_PORT（也就是 192.168.99.100:31380）进行访问，输入 http://192.168.99.100:31380/headers 网址之后，应该会显示浏览器发送的请求 Header。

### 理解原理
Gateway 配置资源允许外部流量进入 Istio 服务网，并使 Istio 的流量管理和策略功能可用于边缘服务。

在前面的步骤中，我们在 Istio 服务网格中创建了一个服务，并展示了如何将服务的 HTTP 端点暴露给外部流量。

### 清理
删除 Gateway 和 VirtualService，并关闭 httpbin 服务：
```
$ istioctl delete gateway httpbin-gateway
$ istioctl delete virtualservice httpbin
$ kubectl delete --ignore-not-found=true -f samples/httpbin/httpbin.yaml
```
___
### 用 HTTPS 加密 Gateway
配置 Istio 令其以 TLS 或双向 TLS 的方式在网格外公开服务。
### 开始之前
1.执行开始之前的步骤，并且确定 Ingress 地址和端口。在完成这些步骤后，应该已经部署了可用的 Istio 服务网格以及 httpbin 应用了，并且 INGRESS_HOST 和 SECURE_INGRESS_PORT 这两个变量也已经生成并赋值。

2.macOS 用户需要注意，要检查一下 curl 的编译是否包含了 LibreSSL 库：
```
$ curl --version | grep LibreSSL
curl 7.54.0 (x86_64-apple-darwin17.0) libcurl/7.54.0 LibreSSL/2.0.20 zlib/1.2.11 nghttp2/1.24.0
```
如果命令输出中包含了 LibreSSL ，那么 curl 命令就是适合于本文任务的。否则就要尝试使用其他的 curl 了，例如使用 Linux 系统。

### 生成客户端与服务器的证书和密钥
这里可以使用任意工具来生成证书和密钥，下面我们使用了一个脚本：

1.克隆 https://github.com/nicholasjackson/mtls-go-example 仓库：

$ git clone https://github.com/nicholasjackson/mtls-go-example

2.进入代码目录：

$ cd mtls-go-example

3.生成证书（任意指定密码）

$ ./generate.sh httpbin.example.com <password>

所有提示问题都选择 y。这一命令会生成四个目录： 1_root、2_intermediate、3_application 以及 4_client，其中包含了后续步骤所需的客户端和服务器的证书。

### 配置 TLS ingress gateway
接下来就要为 Ingress gateway 开放一个 443 端口，用于提供 HTTPS 服务。首先使用密钥和证书作为输入，创建一个 Secret 。然后定义 Gateway 对象，其中包含了一个使用 443 端口的 server。

1.创建一个 Kubernetes sceret 对象，用于保存服务器的证书和私钥。具体说来就是使用 kubectl 命令在命名空间 istio-system 中创建一个 secret 对象，命名为 istio-ingressgateway-certs。Istio 网关会自动载入这个 secret。

这里的 secret 必须 在 istio-system 命名空间中，并且命名为 istio-ingressgateway-certs，否则就不会被正确载入，也就无法在 Istio gateway 中使用了。

$ kubectl create -n istio-system secret tls istio-ingressgateway-certs --key 3_application/private/httpbin.example.com.key.pem --cert 3_application/certs/httpbin.example.com.cert.pem
secret "istio-ingressgateway-certs" created

注意缺省情况下，istio-system 命名空间中所有的 Service account 都是可以访问 Secret 的，所以可能会泄漏私钥。可以通过 RBAC 设置来进行对涉密数据的保护。

2.定义一个 Gateway 对象，其中包含了使用 443 端口的 server 部分。

证书的私钥的位置 必须 是 /etc/istio/ingressgateway-certs，否则 Gateway 无法载入。
```
cat <<EOF | istioctl create -f -
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: httpbin-gateway
spec:
  selector:
    istio: ingressgateway # 使用 Istio 的缺省 Gateway
  servers:
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      serverCertificate: /etc/istio/ingressgateway-certs/tls.crt
      privateKey: /etc/istio/ingressgateway-certs/tls.key
    hosts:
    - "httpbin.example.com"
EOF
```
3.为通过 Gateway 进入的流量进行路由配置。配置一个和控制 Ingress 流量任务 中一致的 Virtualservice：
```
cat <<EOF | istioctl create -f -
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: httpbin
spec:
  hosts:
  - "httpbin.example.com"
  gateways:
  - httpbin-gateway
  http:
  - match:
    - uri:
        prefix: /status
    - uri:
        prefix: /delay
    route:
    - destination:
        port:
          number: 8000
        host: httpbin
EOF
```
4.用 curl 发送 https 请求到 SECURE_INGRESS_PORT，也就是通过 HTTPS 协议访问 httpbin 服务。

--resolve 选项要求 curl 通过域名 httpbin.example.com 使用 TLS 访问 Gateway 地址，这样也就符合了证书的 SNI 要求。--cacert 参数则让 curl 命令使用刚刚生成的证书来对服务器进行校验。

发送请求到 /status/418，会看到漂亮的返回内容，这说明我们成功访问了 httpbin。httpbin 服务会返回 418 I’m a Teapot。
```
$ curl -v --resolve httpbin.example.com:$SECURE_INGRESS_PORT:$INGRESS_HOST --cacert 2_intermediate/certs/ca-chain.cert.pem https://httpbin.example.com:$SECURE_INGRESS_PORT/status/418
...
Server certificate:
  subject: C=US; ST=Denial; L=Springfield; O=Dis; CN=httpbin.example.com
  start date: Jun 24 18:45:18 2018 GMT
  expire date: Jul  4 18:45:18 2019 GMT
  common name: httpbin.example.com (matched)
  issuer: C=US; ST=Denial; O=Dis; CN=httpbin.example.com
SSL certificate verify ok.
...
HTTP/2 418
...
-=[ teapot ]=-

   _...._
 .'  _ _ `.
| ."` ^ `". _,
\_;`"---"`|//
  |       ;/
  \_     _/
    `"""`
```
```
Gateway 定义的传播可能需要一些时间，在传播完成之间的访问，可能会得到这样的错误响应： Failed to connect to httpbin.example.com port <your secure port>: Connection refused。只需等待一分钟，重新访问即可。
```
查看 curl 命令返回内容中的 Server certificate 部分，注意其中的 common name：common name: httpbin.example.com (matched)。另外输出中还包含了 SSL certificate verify ok，这说明对服务器的证书校验是成功的，返回状态码为 418 和一只茶杯犬。

如果需要支持 双向 TLS，请继续下一节内容。

### 配置 Ingress gateway 的双向 TLS 支持
这一节中会再次对 Gateway 定义进行扩展，从而在从外部客户端到 Gateway 的访问中添加对 双向 TLS 的支持。

1.创建一个 Kubernetes secret，用于存储 CA 证书，服务器会使用这一证书来对客户端进行校验。用 kubectl 在 istio-system 命名空间中创建 Secret istio-ingressgateway-ca-certs。Istio gateway 会自动载入这个 Secret。

这个 secret 必须 以 istio-ingressgateway-ca-certs 为名并保存在命名空间 istio-system 之中，否则 Istio gateway 无法正确完成加载过程。
```
$ kubectl create -n istio-system secret generic istio-ingressgateway-ca-certs --from-file=2_intermediate/certs/ca-chain.cert.pem
secret "istio-ingressgateway-ca-certs" created
```
2.重新定义之前的 Gateway，把其中的 tls 一节的 mode 字段的值修改为 MUTUAL，并给 caCertificates 赋值：

证书的位置 必须 是 /etc/istio/ingressgateway-ca-certs，否则 Gateway 无法加载。证书的文件名必须和创建 Secret 时使用的文件名一致，这里就是 ca-chain.cert.pem
```
cat <<EOF | istioctl replace -f -
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: httpbin-gateway
spec:
  selector:
    istio: ingressgateway # 是用缺省的 Istio Gateway
  servers:
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: MUTUAL
      serverCertificate: /etc/istio/ingressgateway-certs/tls.crt
      privateKey: /etc/istio/ingressgateway-certs/tls.key
      caCertificates: /etc/istio/ingressgateway-ca-certs/ca-chain.cert.pem
    hosts:
    - "httpbin.example.com"
EOF
```
3.同样的使用 HTTPS 方式访问 httpbin 服务：
```
$ curl --resolve httpbin.example.com:$SECURE_INGRESS_PORT:$INGRESS_HOST  --cacert 2_intermediate/certs/ca-chain.cert.pem https://httpbin.example.com:$SECURE_INGRESS_PORT/status/418
curl: (35) error:14094410:SSL routines:SSL3_READ_BYTES:sslv3 alert handshake failure
```
Gateway 定义的传播可能需要一些时间，在传播完成之前，可能还会得到 418 的响应。稍事等待后，可再次执行 curl。

因为服务拒绝接受未经验证的请求，这次访问会得到一个错误返回。因此这次调用必须使用客户端证书，并且需要把密钥传递给 curl ，从而完成对请求的签名过程。

4.再次使用 curl 命令发送请求，这次的参数加入了客户端证书（--cert）以及私钥（--key）:
```
$ curl --resolve httpbin.example.com:$SECURE_INGRESS_PORT:$INGRESS_HOST  --cacert 2_intermediate/certs/ca-chain.cert.pem --cert 4_client/certs/httpbin.example.com.cert.pem --key 4_client/private/httpbin.example.com.key.pem https://httpbin.example.com:$SECURE_INGRESS_PORT/status/418
-=[ teapot ]=-

   _...._
 .'  _ _ `.
| ."` ^ `". _,
\_;`"---"`|//
  |       ;/
  \_     _/
    `"""`
```
这次服务器成功校验了客户端证书并放行，因此就再次看到了正确的返回内容。

### 常见问题
查看 INGRESS_HOST 以及 SECURE_INGRESS_PORT 这两个环境变量，确定它们的正确取值，具体命令：
```
$ kubectl get svc -n istio-system
$ echo INGRESS_HOST=$INGRESS_HOST, SECURE_INGRESS_PORT=$SECURE_INGRESS_PORT
```
1.检查 istio-ingressgateway Pod 是否正确的加载了证书和私钥：
```
$ kubectl exec -it -n istio-system $(kubectl -n istio-system get pods -l istio=ingressgateway -o jsonpath='{.items[0].metadata.name}') -- ls -al /etc/istio/ingressgateway-certs
```
tls.crt 和 tls.key 都应该保存在这个目录中。

2.检查 Ingress gateway 证书中的 Subject 字段的正确性：
```
$ kubectl exec -i -n istio-system $(kubectl get pod -l istio=ingressgateway -n istio-system -o jsonpath='{.items[0].metadata.name}')  -- cat /etc/istio/ingressgateway-certs/tls.crt | openssl x509 -text -noout | grep 'Subject:'
    Subject: C=US, ST=Denial, L=Springfield, O=Dis, CN=httpbin.example.com
```
3.检查 Ingress gateway 的代理能够正确访问证书：
```
$ kubectl exec -ti $(kubectl get po -l istio=ingressgateway -n istio-system -o jsonpath={.items[0]..metadata.name}) -n istio-system -- curl  127.0.0.1:15000/certs
{
  "ca_cert": "",
  "cert_chain": "Certificate Path: /etc/istio/ingressgateway-certs/tls.crt, Serial Number: 100212, Days until Expiration: 370"
}
```
4.检查 istio-ingressgateway 中的错误信息：
```
$ kubectl logs -n istio-system -l istio=ingressgateway
```
macOS 用户，检查 curl 是否包含 LibreSSL 库，和开始之前中提到的一样。

### 双向 TLS 常见问题
除去刚才提到的内容之外，执行下列检查：

1.检查 istio-ingressgateway Pod 是否正确加载 CA 证书：
```
$ kubectl exec -it -n istio-system $(kubectl -n istio-system get pods -l istio=ingressgateway -o jsonpath='{.items[0].metadata.name}') -- ls -al /etc/istio/ingressgateway-ca-certs
```
ca-chain.cert.pem 应该保存在这个路径中。

2.检查 Ingress gateway 中 CA 证书的 Subject 字段内容：
```
$ kubectl exec -i -n istio-system $(kubectl get pod -l istio=ingressgateway -n istio-system -o jsonpath='{.items[0].metadata.name}')  -- cat /etc/istio/ingressgateway-ca-certs/ca-chain.cert.pem | openssl x509 -text -noout | grep 'Subject:'
Subject: C=US, ST=Denial, L=Springfield, O=Dis, CN=httpbin.example.com
```
### 清理
1.删除 Gateway 配置、VirtualService 以及 Secret 对象：
```
$ istioctl delete gateway httpbin-gateway
$ istioctl delete virtualservice httpbin
$ kubectl delete --ignore-not-found=true -n istio-system secret istio-ingressgateway-certs istio-ingressgateway-ca-certs
```
2.关闭 httpbin 服务：
```
$ kubectl delete --ignore-not-found=true -f samples/httpbin/httpbin.yaml
```
___
### 控制 Egress 流量
在 Istio 中配置从网格内访问外部服务的流量路由。
### 开始之前
- 根据安装指南的内容，部署 Istio。

- 启动 sleep 示例应用，我们将会使用这一应用来完成对外部服务的调用过程。 如果启用了 Sidecar 的自动注入功能，运行：
```
$ kubectl apply -f samples/sleep/sleep.yaml
```
否则在部署 sleep 应用之前，就需要手工注入 Sidecar：
```
$ kubectl apply -f <(istioctl kube-inject -f samples/sleep/sleep.yaml)
```
实际上任何可以 exec 和 curl 的 Pod 都可以用来完成这一任务。

### 在 Istio 中配置外部服务
通过配置 Istio ServiceEntry，可以从 Istio 集群中访问外部任意的可用服务。这里我们会使用 httpbin.org 以及 www.google.com 进行试验。

### 配置外部服务
1.创建一个 ServiceEntry 对象，放行对一个外部 HTTP 服务的访问：
```
cat <<EOF | kubectl apply -f -
apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
  name: httpbin-ext
spec:
  hosts:
  - httpbin.org
  ports:
  - number: 80
    name: http
    protocol: HTTP
  resolution: DNS
EOF
```
2.创建一个 ServiceEntry 以及 VirtualService，允许访问外部 HTTPS 服务。注意：包括 HTTPS 在内的 TLS 协议，在 ServiceEntry 之外，还需要创建 TLS VirtualService。
```
cat <<EOF | kubectl apply -f -
apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
  name: google
spec:
  hosts:
  - www.google.com
  ports:
  - number: 443
    name: https
    protocol: HTTPS
  resolution: DNS
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: google
spec:
  hosts:
  - www.google.com
  tls:
  - match:
    - port: 443
      sni_hosts:
      - www.google.com
    route:
    - destination:
        host: www.google.com
        port:
          number: 443
      weight: 100
EOF
```
### 发起对外部服务的访问
1.使用 kubectl exec 命令进入测试 Pod。假设使用的是 sleep 服务，运行如下命令：
```
$ export SOURCE_POD=$(kubectl get pod -l app=sleep -o jsonpath={.items..metadata.name})
$ kubectl exec -it $SOURCE_POD -c sleep bash
```
2.发起一个对外部 HTTP 服务的请求：
```
$ curl http://httpbin.org/headers
```
3.发起一个对外部 HTTPS 服务的请求：
```
$ curl https://www.google.com
```
### 为外部服务设置路由规则
通过 ServiceEntry 访问外部服务的流量，和网格内流量类似，都可以进行 Istio 路由规则 的配置。下面我们使用 istioctl 为 httpbin.org 服务设置一个超时规则。

1.在测试 Pod 内部，使用 curl 调用 httpbin.org 这一外部服务的 /delay 端点：
```
$ kubectl exec -it $SOURCE_POD -c sleep bash
$ time curl -o /dev/null -s -w "%{http_code}\n" http://httpbin.org/delay/5
200

real    0m5.024s
user    0m0.003s
sys     0m0.003s
```
这个请求会在大概五秒钟左右返回一个内容为 200 (OK) 的响应。

2.退出测试 Pod，使用 istioctl 为 httpbin.org 外部服务的访问设置一个 3 秒钟的超时：
```
cat <<EOF | istioctl create -f -
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: httpbin-ext
spec:
  hosts:
    - httpbin.org
  http:
  - timeout: 3s
    route:
      - destination:
          host: httpbin.org
        weight: 100
EOF
```
3.等待几秒钟之后，再次发起 curl 请求：
```
$ kubectl exec -it $SOURCE_POD -c sleep bash
$ time curl -o /dev/null -s -w "%{http_code}\n" http://httpbin.org/delay/5
504

real    0m3.149s
user    0m0.004s
sys     0m0.004s
```
这一次会在 3 秒钟之后收到一个内容为 504 (Gateway Timeout) 的响应。虽然 httpbin.org 还在等待他的 5 秒钟，Istio 却在 3 秒钟的时候切断了请求。

### 直接调用外部服务
如果想要跳过 Istio，直接访问某个 IP 范围内的外部服务，就需要对 Envoy sidecar 进行配置，阻止 Envoy 对外部请求的劫持。可以在 Helm 中设置 global.proxy.includeIPRanges 变量，然后使用 kubectl apply 命令来更新名为 istio-sidecar-injector 的 Configmap。在 istio-sidecar-injector 更新之后，global.proxy.includeIPRanges 会在所有未来部署的 Pod 中生效。

使用 global.proxy.includeIPRanges 变量的最简单方式就是把内部服务的 IP 地址范围传递给它，这样就在 Sidecar proxy 的重定向列表中排除掉了外部服务的地址了。

内部服务的 IP 范围取决于集群的部署情况。例如 Minikube 中这一范围是 10.0.0.1/24，这个配置中，就应该这样更新 istio-sidecar-injector：
```
$ helm template install/kubernetes/helm/istio <安装 Istio 时所使用的参数> --set global.proxy.includeIPRanges="10.0.0.1/24" -x templates/sidecar-injector-configmap.yaml | kubectl apply -f -
```
注意这里应该使用和之前部署 Istio 的时候同样的 Helm 命令，尤其是 --namespace 参数。在安装 Istio 原有命令的基础之上，加入 --set global.proxy.includeIPRanges="10.0.0.1/24" -x templates/sidecar-injector-configmap.yaml 即可。

和前面一样，重新部署 sleep 应用。

### 确定 global.proxy.includeIPRanges 的值
根据集群部署情况为 global.proxy.includeIPRanges 赋值。

IBM Cloud Private
从 IBM Cloud Private 配置文件（cluster/config.yaml）中获取 service_cluster_ip_range。

$ cat cluster/config.yaml | grep service_cluster_ip_range

会输出类似内容：

service_cluster_ip_range: 10.0.0.1/24

使用 --set global.proxy.includeIPRanges="10.0.0.1/24"

IBM Cloud Kubernetes Service
使用 --set global.proxy.includeIPRanges="172.30.0.0/16\,172.20.0.0/16\,10.10.10.0/24"

### Minikube
使用 --set global.proxy.includeIPRanges="10.0.0.1/24"

### 访问外部服务
更新了 ConfigMap istio-sidecar-injector 并且重新部署了 sleep 应用之后，Istio sidecar 就应该只劫持和管理集群内部的请求了。任意的外部请求都会简单的绕过 Sidecar，直接访问目的地址。
```
$ export SOURCE_POD=$(kubectl get pod -l app=sleep -o jsonpath={.items..metadata.name})
$ kubectl exec -it $SOURCE_POD -c sleep curl http://httpbin.org/headers
```
### 理解原理
这个任务中，我们使用两种方式从 Istio 服务网格内部来完成对外部服务的调用：

1.使用 ServiceEntry (推荐方式)

2.配置 Istio sidecar，从它的重定向 IP 表中排除外部服务的 IP 范围

第一种方式（ServiceEntry）中，网格内部的服务不论是访问内部还是外部的服务，都可以使用同样的 Istio 服务网格的特性。我们通过为外部服务访问设置超时规则的例子，来证实了这一优势。

第二种方式越过了 Istio sidecar proxy，让服务直接访问到对应的外部地址。然而要进行这种配置，需要了解云供应商特定的知识和配置。

### 清理
1.删除规则：
```
$ kubectl delete serviceentry httpbin-ext google
$ kubectl delete virtualservice httpbin-ext google
```
2.停止 sleep 服务：
```
$ kubectl delete -f samples/sleep/sleep.yaml
```
3.更新 ConfigMap istio-sidecar-injector，要求 Sidecar 转发所有外发流量：
```
$ helm template install/kubernetes/helm/istio <安装 Istio 时所使用的参数> -x templates/sidecar-injector-configmap.yaml | kubectl apply -f -

```
___

### 出口流量的 TLS
此任务描述 Istio 如何配置出口流量的 TLS。
前提条件
- 按照安装指南中的说明设置 Istio 。

- 启动 sleep 示例，它将作为外部调用的测试源。

如果您已启用自动注入 sidecar, 请按如下命令部署 sleep 应用程序:
```
$ kubectl apply -f samples/sleep/sleep.yaml
```
否则，您必须在部署 sleep 应用程序之前手动注入 sidecar：
```
$ kubectl apply -f <(istioctl kube-inject -f samples/sleep/sleep.yaml)
```
请注意，任何可以 exec 和 curl 的 pod 都可以执行以下步骤。

创建一个 shell 变量来保存源 pod 的名称，以便将请求发送到外部服务, 如果您使用 sleep 示例，请按如下命令运行:
```
$ export SOURCE_POD=$(kubectl get pod -l app=sleep -o jsonpath={.items..metadata.name})
```
### 配置 HTTP 和 HTTPS 外部服务
首先，与控制出口流量任务相同的方式配置对 cnn.com 的访问。 请注意，在 hosts 中定义中使用 * 通配符：*.cnn.com , 使用通配符可以访问 www.cnn.com 以及 edition.cnn.com 。

1.创建一个 ServiceEntry 以允许访问外部 HTTP 和 HTTPS 服务：
```
cat <<EOF | istioctl create -f -
apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
  name: cnn
spec:
  hosts:
  - "*.cnn.com"
    ports:
  - number: 80
    name: http-port
    protocol: HTTP
  - number: 443
    name: https-port
    protocol: HTTPS
EOF
```
2.向外部 HTTP 服务发出请求：
```
$ kubectl exec -it $SOURCE_POD -c sleep -- curl -sL -o /dev/null -D - http://edition.cnn.com/politics
HTTP/1.1 301 Moved Permanently
...
location: https://edition.cnn.com/politics
...

HTTP/1.1 200 OK
Content-Type: text/html; charset=utf-8
...
Content-Length: 151654
...
```
输出应该与上面的类似（一些细节用省略号代替）。

注意 curl 的 -L 标志，它指示 curl 遵循重定向, 在这种情况下， 服务器返回一个重定向响应（301 Moved Permanently）到 http://edition.cnn.com/politics 的 HTTP 请求, 重定向响应指示客户端通过 HTTPS 向 https://edition.cnn.com/politics 发送附加请求, 对于第二个请求，服务器返回所请求的内容和 200 OK 状态代码。

而对于 curl 命令，这种重定向是透明的，这里有两个问题, 第一个问题是冗余的第一个请求，它使获取 http://edition.cnn.com/politics 内容的延迟加倍, 第二个问题是 URL 的路径，在这种情况下是 politics ，以明文形式发送, 如果有攻击者嗅探您的应用程序与 cnn.com 之间的通信，则攻击者会知道您的应用程序获取的 cnn.com 的哪些特定主题和文章, 出于隐私原因，您可能希望阻止攻击者披露此类信息。

在下一节中，您将配置 Istio 以执行 TLS 以解决这两个问题, 在继续下一部分之前清理配置：
```
$ istioctl delete serviceentry cnn
```
### 出口流量的 TLS
1.定义一个 ServiceEntry 以允许流量到 edition.cnn.com ，一个 VirtualService 来执行请求端口重写，一个 DestinationRule 用于 TLS 发起。

与上一节中的 ServiceEntry 不同，这里使用 HTTP 作为端口 433 上的协议，因为客户端将发送 HTTP 请求，而 Istio 将为它们执行 TLS 发起, 此外，在此示例中，必须将分辨率设置为 DNS 才能正确配置 Envoy。

最后，请注意 VirtualService 使用特定的主机 edition.cnn.com （没有通配符），因为 Envoy 代理需要确切地知道使用 HTTPS 访问哪个主机：
```
cat <<EOF | istioctl create -f -
apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
  name: cnn
spec:
  hosts:
  - edition.cnn.com
    ports:
  - number: 80
    name: http-port
    protocol: HTTP
  - number: 443
    name: http-port-for-tls-origination
    protocol: HTTP
    resolution: DNS
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: rewrite-port-for-edition-cnn-com
spec:
  hosts:
  - edition.cnn.com
    http:
  - match:
      - port: 80
    route:
    - destination:
        host: edition.cnn.com
        port:
          number: 443
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: originate-tls-for-edition-cnn-com
spec:
  host: edition.cnn.com
  trafficPolicy:
    loadBalancer:
      simple: ROUND_ROBIN
    portLevelSettings:
    - port:
        number: 443
      tls:
        mode: SIMPLE # initiates HTTPS when accessing edition.cnn.com
EOF
```
2.发送 HTTP 请求到 http://edition.cnn.com/politics ，如上一节所述：
```
$ kubectl exec -it $SOURCE_POD -c sleep -- curl -sL -o /dev/null -D - http://edition.cnn.com/politics
HTTP/1.1 200 OK
Content-Type: text/html; charset=utf-8
...
Content-Length: 151654
...
```
这次你收到 200 OK , Istio 为 curl 执行了 TLS 发起，因此原始 HTTP 请求作为 HTTPS 转发到 cnn.com , cnn.com 服务器直接返回内容，无需重定向, 您消除了客户端和服务器之间的双重往返，并且请求使网格加密，而没有透露应用程序获取 cnn.com 的 politics 部分这一事实。

请注意，您使用的命令与上一节中的命令相同, 对于以编程方式访问外部服务的应用程序，代码不会更改, 因此，您可以通过配置 Istio 来获得 TLS 的好处，而无需更改代码行。

其他安全因素
请注意，应用程序 pid 与本地主机上的 sidecar 之间的流量仍未加密, 这意味着如果攻击者能够穿透应用程序的节点，他们仍然可以在节点的本地网络上看到未加密的通信, 在某些环境中，可能存在严格的安全要求，即必须加密所有流量，即使在节点的本地网络上也是如此, 如果有这么严格的要求，应用程序应该只使用 HTTPS（TLS），此任务中描述的 TLS 是不够的。

另请注意，即使对于应用程序发起的 HTTPS ，攻击者也可以通过检查服务器名称指示（SNI）来了解对 cnn.com 的请求, ）, 在 TLS 握手期间，未加密地发送 SNI 字段, 使用 HTTPS 可防止攻击者了解特定主题和文章，但这并不能阻止攻击者了解 cnn.com 被访问。

### 清理
1.删除您创建的 Istio 配置项：
```
$ istioctl delete serviceentry cnn
$ istioctl delete virtualservice rewrite-port-for-edition-cnn-com
$ istioctl delete destinationrule originate-tls-for-edition-cnn-com
```
2.关闭 sleep 服务：
```
$ kubectl delete -f samples/sleep/sleep.yaml

```
___
### 配置 Egress 网关
描述如何通过专用网关服务将流量定向到外部服务来配置 Istio。
开始之前
- 按照安装指南中的说明设置 Istio 。

- 启动 sleep，它将被用作外部调用的测试源。

如果您已经启用了 automatic sidecar injection，请执行此操作
```
$ kubectl apply -f samples/sleep/sleep.yaml
```
否则，您必须在部署 sleep 应用程序之前手动注入 sidecar：
```
$ kubectl apply -f <(istioctl kube-inject -f samples/sleep/sleep.yaml)
```
请注意，您可以在任意 pod 使用 exec 和 curl。

创建一个 shell 变量来保存源 pod 的名称，以便将请求发送到外部服务。如果我们使用 sleep 示例，我们运行：
```
$ export SOURCE_POD=$(kubectl get pod -l app=sleep -o jsonpath={.items..metadata.name})
```
### 定义 egress Gateway 并通过它定向 HTTP 流量
首先定向没有 TLS 的 HTTP 流量

1.为 edition.cnn.com 定义一个 ServiceEntry：
```
cat <<EOF | istioctl create -f -
apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
  name: cnn
spec:
  hosts:
  - edition.cnn.com
    ports:
  - number: 80
    name: http-port
    protocol: HTTP
  - number: 443
    name: https
    protocol: HTTPS
    resolution: DNS
EOF
```
2.验证您的 ServiceEntry 是否已正确应用。发送 HTTPS 请求到 http://edition.cnn.com/politics。
```
$ kubectl exec -it $SOURCE_POD -c sleep -- curl -sL -o /dev/null -D - http://edition.cnn.com/politics
HTTP/1.1 301 Moved Permanently
...
location: https://edition.cnn.com/politics
...

HTTP/1.1 200 OK
Content-Type: text/html; charset=utf-8
...
Content-Length: 151654
...
```
输出应与 Egress 流量的 TLS 任务中的输出相同，不带 TLS。

3.创建 egress Gateway 为 edition.cnn.com ，端口 80。

如果在 Istio 中启用了双向 TLS 认证，请使用以下命令。请注意，除了创建 Gateway 之外，它还创建了一个 DestinationRule 来指定 egress 网关的 双向 TLS，将 SNI 设置为 edition.cnn.com。
```
cat <<EOF | istioctl create -f -
  kind: Gateway
  metadata:
    name: istio-egressgateway
  spec:
    selector:
      istio: egressgateway
    servers:
    - port:
        number: 80
        name: https
        protocol: HTTPS
      hosts:
      - edition.cnn.com
      tls:
        mode: MUTUAL
        serverCertificate: /etc/certs/cert-chain.pem
        privateKey: /etc/certs/key.pem
        caCertificates: /etc/certs/root-cert.pem
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: set-sni-for-egress-gateway
spec:
  host: istio-egressgateway.istio-system.svc.cluster.local
  trafficPolicy:
    loadBalancer:
      simple: ROUND_ROBIN
    portLevelSettings:
    - port:
        number: 80
      tls:
        mode: MUTUAL
        clientCertificate: /etc/certs/cert-chain.pem
        privateKey: /etc/certs/key.pem
        caCertificates: /etc/certs/root-cert.pem
        subjectAltNames:
        - spiffe://cluster.local/ns/istio-system/sa/istio-egressgateway-service-account
        sni: edition.cnn.com
EOF
```
除此之外：
```
cat <<EOF | istioctl create -f -
  kind: Gateway
  metadata:
    name: istio-egressgateway
  spec:
    selector:
      istio: egressgateway
    servers:
    - port:
        number: 80
        name: http
        protocol: HTTP
      hosts:
      - edition.cnn.com
EOF
```
4.定义 VirtualService 来引导流量通过 egress 网关：
```
cat <<EOF | istioctl create -f -
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: direct-through-egress-gateway
spec:
  hosts:
  - edition.cnn.com
    gateways:
  - istio-egressgateway
  - mesh
    http:
  - match:
    - gateways:
      - mesh
      port: 80
    route:
    - destination:
        host: istio-egressgateway.istio-system.svc.cluster.local
        port:
          number: 80
        weight: 100
  - match:
    - gateways:
      - istio-egressgateway
      port: 80
    route:
    - destination:
        host: edition.cnn.com
        port:
          number: 80
        weight: 100
EOF
```
5.将 HTTP 请求重新发送到 http://edition.cnn.com/politics。
```
$ kubectl exec -it $SOURCE_POD -c sleep -- curl -sL -o /dev/null -D - http://edition.cnn.com/politics
HTTP/1.1 301 Moved Permanently
...
location: https://edition.cnn.com/politics
...

HTTP/1.1 200 OK
Content-Type: text/html; charset=utf-8
...
Content-Length: 151654
...
```
输出应与步骤2中的输出相同。

6.检查 istio-egressgateway pod 的日志，并查看与我们的请求对应的行。如果 Istio 部署在 istio-system 命名空间中，则打印日志的命令是：
```
$ kubectl logs $(kubectl get pod -l istio=egressgateway -n istio-system -o jsonpath='{.items[0].metadata.name}') egressgateway -n istio-system | tail
```
我们看到与请求相关的行，类似于以下内容：
```
[2018-06-14T11:46:23.596Z] "GET /politics HTTP/1.1" 301 - 0 0 3 1 "172.30.146.87" "curl/7.35.0" "ab7be694-e367-94c5-83d1-086eca996dae" "edition.cnn.com" "151.101.193.67:80"
```
请注意，我们只将流量从 80 端口重定向到 egress 网关，到 443 端口的 HTTPS 流量直接转到 edition.cnn.com 。

### 清除 HTTP 流量的 egress 网关
在继续下一步之前删除先前的定义：
```
$ istioctl delete gateway istio-egressgateway
$ istioctl delete serviceentry cnn
$ istioctl delete virtualservice direct-through-egress-gateway
$ istioctl delete destinationrule set-sni-for-egress-gateway
```
### Egress Gateway 执行 TLS
让我们用 egress Gateway 执行 TLS，类似于 TLS Origination for Egress Traffic 任务。请注意，在这种情况下，TLS 将由 egress 网关服务器完成，而不是前一任务中的 sidecar。

1.为 edition.cnn.com 定义 ServiceEntry：
```
cat <<EOF | istioctl create -f -
apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
  name: cnn
spec:
  hosts:
  - edition.cnn.com
    ports:
  - number: 80
    name: http-port
    protocol: HTTP
  - number: 443
    name: http-port-for-tls-origination
    protocol: HTTP
    resolution: DNS
EOF
```
2.验证您的 ServiceEntry 是否已正确应用。发送 HTTPS 请求到 http://edition.cnn.com/politics。
```
$ kubectl exec -it $SOURCE_POD -c sleep -- curl -sL -o /dev/null -D - http://edition.cnn.com/politics
HTTP/1.1 301 Moved Permanently
...
location: https://edition.cnn.com/politics
...

command terminated with exit code 35
```
输出应该包含 301 Moved Permanently ，如果您看到它，证明 ServiceEntry 配置正确。退出代码 35 是由于 Istio 没有执行 TLS。 Egress 网关将执行 TLS，继续执行以下步骤进行配置。

3.为 edition.cnn.com 创建 egress Gateway，端口 443。

如果在 Istio 中启用了 双向 TLS 认证 ，请使用以下命令。请注意，除了创建 Gateway 之外，它还创建了一个 DestinationRule 来指定 egress 网关的 双向 TLS，将 SNI 设置为 edition.cnn.com。
```
cat <<EOF | istioctl create -f -
kind: Gateway
metadata:
  name: istio-egressgateway
spec:
  selector:
    istio: egressgateway
  servers:
  - port:
      number: 443
      name: https
      protocol: HTTPS
    hosts:
    - edition.cnn.com
    tls:
      mode: MUTUAL
      serverCertificate: /etc/certs/cert-chain.pem
      privateKey: /etc/certs/key.pem
      caCertificates: /etc/certs/root-cert.pem
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: set-sni-for-egress-gateway
spec:
  host: istio-egressgateway.istio-system.svc.cluster.local
  trafficPolicy:
    loadBalancer:
      simple: ROUND_ROBIN
    portLevelSettings:
    - port:
        number: 443
      tls:
        mode: MUTUAL
        clientCertificate: /etc/certs/cert-chain.pem
        privateKey: /etc/certs/key.pem
        caCertificates: /etc/certs/root-cert.pem
        subjectAltNames:
        - spiffe://cluster.local/ns/istio-system/sa/istio-egressgateway-service-account
        sni: edition.cnn.com
EOF
```
除此之外：
```
cat <<EOF | istioctl create -f -
kind: Gateway
metadata:
  name: istio-egressgateway
spec:
  selector:
    istio: egressgateway
  servers:
  - port:
      number: 443
      name: http-port-for-tls-origination
      protocol: HTTP
    hosts:
    - edition.cnn.com
EOF
```
4.定义 VirtualService 来引导流量通过 egress 网关，并定义 DestinationRule 以执行 TLS：
```
cat <<EOF | istioctl create -f -
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: direct-through-egress-gateway
spec:
  hosts:
  - edition.cnn.com
    gateways:
  - istio-egressgateway
  - mesh
    http:
  - match:
    - gateways:
      - mesh
      port: 80
    route:
    - destination:
        host: istio-egressgateway.istio-system.svc.cluster.local
        port:
          number: 443
        weight: 100
  - match:
    - gateways:
      - istio-egressgateway
      port: 443
    route:
    - destination:
        host: edition.cnn.com
        port:
          number: 443
        weight: 100
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: originate-tls-for-edition-cnn-com
spec:
  host: edition.cnn.com
  trafficPolicy:
    loadBalancer:
      simple: ROUND_ROBIN
    portLevelSettings:
    - port:
        number: 443
      tls:
        mode: SIMPLE # initiates HTTPS for connections to edition.cnn.com
EOF
```
5.发送 HTTP 请求到 http://edition.cnn.com/politics.
```
$ kubectl exec -it $SOURCE_POD -c sleep -- curl -sL -o /dev/null -D - http://edition.cnn.com/politics
HTTP/1.1 200 OK
...
content-length: 150793
...
```
输出应与 TLS Origination for Egress Traffic 任务中的输出相同，TLS 来源：没有 301 Moved Permanently 信息。

6.检查 istio-egressgateway pod 的日志，并查看与我们的请求相对应的行。如果 Istio 部署在 istio-system 命名空间中，则打印日志的命令是：
```
$ kubectl logs $(kubectl get pod -l istio=egressgateway -n istio-system -o jsonpath='{.items[0].metadata.name}') egressgateway -n istio-system | tail
```
我们看到与我们的请求相关的行，类似于以下内容：
```
"[2018-06-14T13:49:36.340Z] "GET /politics HTTP/1.1" 200 - 0 148528 5096 90 "172.30.146.87" "curl/7.35.0" "c6bfdfc3-07ec-9c30-8957-6904230fd037" "edition.cnn.com" "151.101.65.67:443"
```
### 清除 TLS 发起的 egress 网关
删除我们创建的 Istio 配置项：
```
$ istioctl delete gateway istio-egressgateway
$ istioctl delete serviceentry cnn
$ istioctl delete virtualservice direct-through-egress-gateway
$ istioctl delete destinationrule originate-tls-for-edition-cnn-com
$ istioctl delete destinationrule set-sni-for-egress-gateway
```
### 通过 egress 网关定向 HTTPS 流量
在本节中，您将通过 egress 网关引导HTTPS流量（由应用程序发起的 TLS）。在相应的 ServiceEntry 中指定端口 443，协议 TLS，egress Gateway 和 VirtualService。

1.为 edition.cnn.com 定义 ServiceEntry：
```
cat <<EOF | istioctl create -f -
apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
  name: cnn
spec:
  hosts:
  - edition.cnn.com
    ports:
  - number: 443
    name: tls
    protocol: TLS
    resolution: DNS
EOF
```
2.验证您的 ServiceEntry 是否已正确应用。发送 HTTPS 请求到 http://edition.cnn.com/politics。输出应与上一节中的输出相同。
```
$ kubectl exec -it $SOURCE_POD -c sleep -- curl -sL -o /dev/null -D - https://edition.cnn.com/politics
HTTP/1.1 200 OK
Content-Type: text/html; charset=utf-8
...
Content-Length: 151654
...
```
3.为 edition.cnn.com 创建 egress Gateway ，端口 443，TLS 协议。

如果在 Istio 中启用了双向 TLS 认证 ，请使用以下命令。请注意，除了创建 Gateway 之外，它还创建了一个 DestinationRule 来指定 egress 网关的 双向 TLS，将 SNI 设置为 edition.cnn.com。
```
cat <<EOF | istioctl create -f -
kind: Gateway
metadata:
  name: istio-egressgateway
spec:
  selector:
    istio: egressgateway
  servers:
  - port:
      number: 443
      name: tls-cnn
      protocol: TLS
    hosts:
    - edition.cnn.com
    tls:
      mode: MUTUAL
      serverCertificate: /etc/certs/cert-chain.pem
      privateKey: /etc/certs/key.pem
      caCertificates: /etc/certs/root-cert.pem
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: set-sni-for-egress-gateway
spec:
  host: istio-egressgateway.istio-system.svc.cluster.local
  trafficPolicy:
    loadBalancer:
      simple: ROUND_ROBIN
    portLevelSettings:
    - port:
        number: 443
      tls:
        mode: MUTUAL
        clientCertificate: /etc/certs/cert-chain.pem
        privateKey: /etc/certs/key.pem
        caCertificates: /etc/certs/root-cert.pem
        subjectAltNames:
        - spiffe://cluster.local/ns/istio-system/sa/istio-egressgateway-service-account
        sni: edition.cnn.com
EOF
```
除此之外:
```
cat <<EOF | istioctl create -f -
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: istio-egressgateway
spec:
  selector:
    istio: egressgateway
  servers:
  - port:
      number: 443
      name: tls
      protocol: TLS
    hosts:
    - edition.cnn.com
    tls:
      mode: PASSTHROUGH
EOF
```
4.定义 VirtualService 来引导流量通过 egress 网关：
```
cat <<EOF | istioctl create -f -
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: direct-through-egress-gateway
spec:
  hosts:
  - edition.cnn.com
    gateways:
  - istio-egressgateway
  - mesh
    tls:
  - match:
    - gateways:
      - mesh
      port: 443
      sni_hosts:
      - edition.cnn.com
    route:
    - destination:
        host: istio-egressgateway.istio-system.svc.cluster.local
        port:
          number: 443
        weight: 100
  - match:
    - gateways:
      - istio-egressgateway
      port: 443
      sni_hosts:
      - edition.cnn.com
    route:
    - destination:
        host: edition.cnn.com
        port:
          number: 443
        weight: 100
EOF
```
5.发送 HTTPS 请求到 http://edition.cnn.com/politics。输出应与之前相同。
```
$ kubectl exec -it $SOURCE_POD -c sleep -- curl -sL -o /dev/null -D - https://edition.cnn.com/politics
HTTP/1.1 200 OK
Content-Type: text/html; charset=utf-8
...
Content-Length: 151654
...
```
6.检查 egress 网关代理的统计信息，并查看与我们对 edition.cnn.com 的请求相对应的计数器。如果 Istio 部署在 istio-system 命名空间中，则打印计数器的命令是：
```
$ kubectl exec -it $(kubectl get pod -l istio=egressgateway -n istio-system -o jsonpath='{.items[0].metadata.name}') -c egressgateway -n istio-system -- curl -s localhost:15000/stats | grep edition.cnn.com.upstream_cx_total
cluster.outbound|443||edition.cnn.com.upstream_cx_total: 1
```
您可能需要执行几个额外的请求，并验证每个请求上面的计数器增加1。

### 清除 HTTPS 流量的 egress 网关
```
$ istioctl delete serviceentry cnn
$ istioctl delete gateway istio-egressgateway
$ istioctl delete virtualservice direct-through-egress-gateway
$ istioctl delete destinationrule set-sni-for-egress-gateway
```
### 其他安全因素
请注意，在 Istio 中定义 egress Gateway 本身并不为运行 egress 网关服务的节点提供任何特殊处理。集群管理员或云提供商可以在专用节点上部署 egress 网关，并引入额外的安全措施，使这些节点比网格的其余部分更安全。

另请注意，实际上 Istio 本身无法安全地强制将所有 egress 流量流经 egress 网关，Istio 仅通过其 sidecar 代理启用此类流量。如果恶意应用程序攻击连接到应用程序 pod 的 sidecar 代理，它可能会绕过 sidecar 代理。绕过 sidecar 代理后，恶意应用程序可能会尝试绕过 egress 网关退出服务网格，以逃避 Istio 的控制和监控。由集群管理员或云提供商来强制执行没有流量绕过 egress 网关的网格。这种强制执行必须由 Istio 以外的机制执行。例如，防火墙可以拒绝其源不是 egress 网关的所有流量。 Kubernetes 网络策略也可以禁止所有不在 egress 网关中出口的 egress 流量。另一种可能的安全措施涉及以这样的方式配置网络：应用节点不能访问因特网而不将 egress 流量引导到将被监视和控制的网关。这种网络配置的一个例子是将公共 IP 专门分配给网关。

### 故障排除
1.检查您是否在 Istio 中启用了双向 TLS 认证，然后执行以下步骤：验证 Istio 的双向 TLS 认证设置。如果启用了双向 TLS，请确保创建相应的项目配置（请注意备注 如果您在 Istio 中启用了双向 TLS 认证，则必须创建… ）。

2.如果双向 TLS 认证启用后, 验证 egress 网关的证书：
```
$ kubectl exec -i -n istio-system $(kubectl get pod -l istio=egressgateway -n istio-system -o jsonpath='{.items[0].metadata.name}')  -- cat /etc/certs/cert-chain.pem | openssl x509 -text -noout  | grep 'Subject Alternative Name' -A 1
        X509v3 Subject Alternative Name:
            URI:spiffe://cluster.local/ns/istio-system/sa/istio-egressgateway-service-account

```
### 清理
关闭 sleep 服务:
```
$ kubectl delete -f samples/sleep/sleep.yaml
```
___
### 熔断
演示弹性应用所需的熔断能力。
### 开始之前
- 跟随安装指南 设置 Istio。
- 启动 httpbin 示例应用，这个应用将会作为本任务的后端服务。

如果启用了 Sidecar 的自动注入，只需运行：
```
$ kubectl apply -f @samples/httpbin/httpbin.yaml
```
否者就需要在部署 httpbin 应用之前手工注入 Sidecar 了：
```
$ kubectl apply -f <(istioctl kube-inject -f samples/httpbin/httpbin.yaml)
```
### 断路器
接下来设置一个场景来演示 Istio 的熔断功能。在上一步骤中，我们已经启动了 httpbin 服务。

1.创建一个 目标规则，针对 httpbin 服务设置断路器：
```
cat <<EOF | istioctl create -f -
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: httpbin
spec:
  host: httpbin
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 1
      http:
        http1MaxPendingRequests: 1
        maxRequestsPerConnection: 1
    outlierDetection:
      consecutiveErrors: 1
      interval: 1s
      baseEjectionTime: 3m
      maxEjectionPercent: 100
EOF
```
2.检查我们的目标规则，确定已经正确建立：
```
$ istioctl get destinationrule httpbin -o yaml
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: httpbin
  ...
spec:
  host: httpbin
  trafficPolicy:
    connectionPool:
      http:
        http1MaxPendingRequests: 1
        maxRequestsPerConnection: 1
      tcp:
        maxConnections: 1
    outlierDetection:
      baseEjectionTime: 180.000s
      consecutiveErrors: 1
      interval: 1.000s
      maxEjectionPercent: 100
```      
### 设置客户端
现在我们已经设置了调用 httpbin 服务的规则，接下来创建一个客户端，用来向后端服务发送请求，观察是否会触发熔断策略。这里要使用一个简单的负载测试客户端，名字叫 fortio。这个客户端可以控制连接数量、并发数以及发送 HTTP 请求的延迟。这里我们会把给客户端也进行 Sidecar 的注入，以此保证 Istio 对网络交互的控制：

$ kubectl apply -f <(istioctl kube-inject -f samples/httpbin/sample-client/fortio-deploy.yaml@)
接下来就可以登入客户端 Pod 并使用 Fortio 工具来调用 httpbin。-curl 参数表明只调用一次：
```
$ FORTIO_POD=$(kubectl get pod | grep fortio | awk '{ print $1 }')
$ kubectl exec -it $FORTIO_POD  -c fortio /usr/local/bin/fortio -- load -curl  http://httpbin:8000/get
HTTP/1.1 200 OK
server: envoy
date: Tue, 16 Jan 2018 23:47:00 GMT
content-type: application/json
access-control-allow-origin: *
access-control-allow-credentials: true
content-length: 445
x-envoy-upstream-service-time: 36

{
  "args": {},
  "headers": {
    "Content-Length": "0",
    "Host": "httpbin:8000",
    "User-Agent": "istio/fortio-0.6.2",
    "X-B3-Sampled": "1",
    "X-B3-Spanid": "824fbd828d809bf4",
    "X-B3-Traceid": "824fbd828d809bf4",
    "X-Ot-Span-Context": "824fbd828d809bf4;824fbd828d809bf4;0000000000000000",
    "X-Request-Id": "1ad2de20-806e-9622-949a-bd1d9735a3f4"
  },
  "origin": "127.0.0.1",
  "url": "http://httpbin:8000/get"
}
```
不难看出，调用已经成功。接下来做些变化。

### 触发熔断机制
在上面的熔断设置中指定了 maxConnections: 1 以及 http1MaxPendingRequests: 1。这意味着如果超过了一个连接同时发起请求，Istio 就会熔断，阻止后续的请求或连接。接下来尝试一下两个并发连接（-c 2），发送 20 请求（-n 20）：
```
$ kubectl exec -it $FORTIO_POD  -c fortio /usr/local/bin/fortio -- load -c 2 -qps 0 -n 20 -loglevel Warning http://httpbin:8000/get
Fortio 0.6.2 running at 0 queries per second, 2->2 procs, for 5s: http://httpbin:8000/get
Starting at max qps with 2 thread(s) [gomax 2] for exactly 20 calls (10 per thread + 0)
23:51:10 W http.go:617> Parsed non ok code 503 (HTTP/1.1 503)
Ended after 106.474079ms : 20 calls. qps=187.84
Aggregated Function Time : count 20 avg 0.010215375 +/- 0.003604 min 0.005172024 max 0.019434859 sum 0.204307492
# range, mid point, percentile, count
>= 0.00517202 <= 0.006 , 0.00558601 , 5.00, 1
> 0.006 <= 0.007 , 0.0065 , 20.00, 3
> 0.007 <= 0.008 , 0.0075 , 30.00, 2
> 0.008 <= 0.009 , 0.0085 , 40.00, 2
> 0.009 <= 0.01 , 0.0095 , 60.00, 4
> 0.01 <= 0.011 , 0.0105 , 70.00, 2
> 0.011 <= 0.012 , 0.0115 , 75.00, 1
> 0.012 <= 0.014 , 0.013 , 90.00, 3
> 0.016 <= 0.018 , 0.017 , 95.00, 1
> 0.018 <= 0.0194349 , 0.0187174 , 100.00, 1
# target 50% 0.0095
# target 75% 0.012
# target 99% 0.0191479
# target 99.9% 0.0194062
Code 200 : 19 (95.0 %)
Code 503 : 1 (5.0 %)
Response Header Sizes : count 20 avg 218.85 +/- 50.21 min 0 max 231 sum 4377
Response Body/Total Sizes : count 20 avg 652.45 +/- 99.9 min 217 max 676 sum 13049
All done 20 calls (plus 0 warmup) 10.215 ms avg, 187.8 qps
```
这里可以看到，几乎所有请求都通过了。
```
Code 200 : 19 (95.0 %)
Code 503 : 1 (5.0 %)
```
Istio-proxy 允许一些余地。接下来把并发连接数量提高到 3：
```
$ kubectl exec -it $FORTIO_POD  -c fortio /usr/local/bin/fortio -- load -c 3 -qps 0 -n 20 -loglevel Warning http://httpbin:8000/get
Fortio 0.6.2 running at 0 queries per second, 2->2 procs, for 5s: http://httpbin:8000/get
Starting at max qps with 3 thread(s) [gomax 2] for exactly 30 calls (10 per thread + 0)
23:51:51 W http.go:617> Parsed non ok code 503 (HTTP/1.1 503)
23:51:51 W http.go:617> Parsed non ok code 503 (HTTP/1.1 503)
23:51:51 W http.go:617> Parsed non ok code 503 (HTTP/1.1 503)
23:51:51 W http.go:617> Parsed non ok code 503 (HTTP/1.1 503)
23:51:51 W http.go:617> Parsed non ok code 503 (HTTP/1.1 503)
23:51:51 W http.go:617> Parsed non ok code 503 (HTTP/1.1 503)
23:51:51 W http.go:617> Parsed non ok code 503 (HTTP/1.1 503)
23:51:51 W http.go:617> Parsed non ok code 503 (HTTP/1.1 503)
23:51:51 W http.go:617> Parsed non ok code 503 (HTTP/1.1 503)
23:51:51 W http.go:617> Parsed non ok code 503 (HTTP/1.1 503)
23:51:51 W http.go:617> Parsed non ok code 503 (HTTP/1.1 503)
Ended after 71.05365ms : 30 calls. qps=422.22
Aggregated Function Time : count 30 avg 0.0053360199 +/- 0.004219 min 0.000487853 max 0.018906468 sum 0.160080597
# range, mid point, percentile, count
>= 0.000487853 <= 0.001 , 0.000743926 , 10.00, 3
> 0.001 <= 0.002 , 0.0015 , 30.00, 6
> 0.002 <= 0.003 , 0.0025 , 33.33, 1
> 0.003 <= 0.004 , 0.0035 , 40.00, 2
> 0.004 <= 0.005 , 0.0045 , 46.67, 2
> 0.005 <= 0.006 , 0.0055 , 60.00, 4
> 0.006 <= 0.007 , 0.0065 , 73.33, 4
> 0.007 <= 0.008 , 0.0075 , 80.00, 2
> 0.008 <= 0.009 , 0.0085 , 86.67, 2
> 0.009 <= 0.01 , 0.0095 , 93.33, 2
> 0.014 <= 0.016 , 0.015 , 96.67, 1
> 0.018 <= 0.0189065 , 0.0184532 , 100.00, 1
# target 50% 0.00525
# target 75% 0.00725
# target 99% 0.0186345
# target 99.9% 0.0188793
Code 200 : 19 (63.3 %)
Code 503 : 11 (36.7 %)
Response Header Sizes : count 30 avg 145.73333 +/- 110.9 min 0 max 231 sum 4372
Response Body/Total Sizes : count 30 avg 507.13333 +/- 220.8 min 217 max 676 sum 15214
All done 30 calls (plus 0 warmup) 5.336 ms avg, 422.2 qps
```
这时候会观察到，熔断行为按照之前的设计生效了：
```
Code 200 : 19 (63.3 %)
Code 503 : 11 (36.7 %)
```
只有 63.3% 的请求获得通过，剩余请求被断路器拦截了。我们可以查询 istio-proxy 的状态，获取更多相关信息：
```
$ kubectl exec -it $FORTIO_POD  -c istio-proxy  -- sh -c 'curl localhost:15000/stats' | grep httpbin | grep pending
cluster.outbound|80||httpbin.springistio.svc.cluster.local.upstream_rq_pending_active: 0
cluster.outbound|80||httpbin.springistio.svc.cluster.local.upstream_rq_pending_failure_eject: 0
cluster.outbound|80||httpbin.springistio.svc.cluster.local.upstream_rq_pending_overflow: 12
cluster.outbound|80||httpbin.springistio.svc.cluster.local.upstream_rq_pending_total: 39
```
upstream_rq_pending_overflow 的值是 12，说明有 12 次调用被标志为熔断。

### 清理
1.清理规则
```
$ istioctl delete destinationrule httpbin
```
2.关闭 httpbin 服务和客户端
```
$ kubectl delete deploy httpbin fortio-deploy
$ kubectl delete svc httpbin
```
___
### 镜像
此任务演示了 Istio 的流量镜像/阴影功能。
### 开始之前
- 按照安装指南中的说明设置 Istio 。

- 首先部署启用了访问日志的两个版本的 httpbin 服务：

httpbin-v1:
```
cat <<EOF | istioctl kube-inject -f - | kubectl create -f -
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: httpbin-v1
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: httpbin
        version: v1
    spec:
      containers:
      - image: docker.io/kennethreitz/httpbin
        imagePullPolicy: IfNotPresent
        name: httpbin
        command: ["gunicorn", "--access-logfile", "-", "-b", "0.0.0.0:8080", "httpbin:app"]
        ports:
        - containerPort: 8080
EOF
```
httpbin-v2:
```
cat <<EOF | istioctl kube-inject -f - | kubectl create -f -
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: httpbin-v2
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: httpbin
        version: v2
    spec:
      containers:
      - image: docker.io/kennethreitz/httpbin
        imagePullPolicy: IfNotPresent
        name: httpbin
        command: ["gunicorn", "--access-logfile", "-", "-b", "0.0.0.0:8080", "httpbin:app"]
        ports:
        - containerPort: 8080
EOF
```
httpbin Kubernetes service:
```
cat <<EOF | kubectl create -f -
apiVersion: v1
kind: Service
metadata:
  name: httpbin
  labels:
    app: httpbin
spec:
  ports:
  - name: http
    port: 8080
  selector:
    app: httpbin
EOF
```
启动 sleep 服务，这样您就可以使用 curl 来提供负载：

sleep service:
```
cat <<EOF | istioctl kube-inject -f - | kubectl create -f -
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: sleep
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: sleep
    spec:
      containers:
      - name: sleep
        image: tutum/curl
        command: ["/bin/sleep","infinity"]
        imagePullPolicy: IfNotPresent
EOF
```
### 创建默认路由策略
默认情况下，Kubernetes 在 httpbin 服务的两个版本之间进行负载均衡。在此步骤中，您将更改该行为，以便所有流量都转到 v1 。

1.创建一个默认路由规则，将所有流量路由到服务的 v1 ：
```
cat <<EOF | istioctl create -f -
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: httpbin
spec:
  hosts:
    - httpbin
  http:
  - route:
    - destination:
        host: httpbin
        subset: v1
      weight: 100
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: httpbin
spec:
  host: httpbin
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
EOF
```
注意：如果您已经安装/配置 Istio 并启用 TLS 双向认证，您必须增加 TLSSettings.TLSmode, mode: ISTIO_MUTUAL 。如 TLSSettings 参考中所述。

现在所有流量已经都转到 httpbin v1 服务。

2.向服务发送一些流量：
```
$ export SLEEP_POD=$(kubectl get pod -l app=sleep -o jsonpath={.items..metadata.name})
$ kubectl exec -it $SLEEP_POD -c sleep -- sh -c 'curl  http://httpbin:8080/headers' | python -m json.tool
{
  "headers": {
    "Accept": "*/*",
    "Content-Length": "0",
    "Host": "httpbin:8080",
    "User-Agent": "curl/7.35.0",
    "X-B3-Sampled": "1",
    "X-B3-Spanid": "eca3d7ed8f2e6a0a",
    "X-B3-Traceid": "eca3d7ed8f2e6a0a",
    "X-Ot-Span-Context": "eca3d7ed8f2e6a0a;eca3d7ed8f2e6a0a;0000000000000000"
  }
}
```
3.查看 httpbin pods 的 v1 和 v2 日志。您可以看到 v1 的访问日志和 v2 为 <none> 的日志：
```
$ export V1_POD=$(kubectl get pod -l app=httpbin,version=v1 -o jsonpath={.items..metadata.name})
$ kubectl logs -f $V1_POD -c httpbin
127.0.0.1 - - [07/Mar/2018:19:02:43 +0000] "GET /headers HTTP/1.1" 200 321 "-" "curl/7.35.0"
```
```
$ export V2_POD=$(kubectl get pod -l app=httpbin,version=v2 -o jsonpath={.items..metadata.name})
$ kubectl logs -f $V2_POD -c httpbin
<none>
```
### 镜像流量到 v2
1.改变路由规则将流量镜像到 v2:
```
cat <<EOF | istioctl replace -f -
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: httpbin
spec:
  hosts:
    - httpbin
  http:
  - route:
    - destination:
        host: httpbin
        subset: v1
      weight: 100
    mirror:
      host: httpbin
      subset: v2
EOF
```
此路由规则将 100％ 的流量发送到 v1 。最后一节指定镜像到 httpbin v2 服务。当流量被镜像时，请求将通过其主机/授权报头发送到镜像服务附上 -shadow 。例如，将 cluster-1 变为 cluster-1-shadow 。

此外，重点注意这些请求被镜像为”即发即弃”，这意味着这些响应是被丢弃的。

2.发送流量：
```
$ kubectl exec -it $SLEEP_POD -c sleep -- sh -c 'curl  http://httpbin:8080/headers' | python -m json.tool
```
现在，您可以查看 v1 和 v2 的访问日志记录。在 v2 中创建的请求实际上也通过了 v1 。
```
$ kubectl logs -f $V1_POD -c httpbin
127.0.0.1 - - [07/Mar/2018:19:02:43 +0000] "GET /headers HTTP/1.1" 200 321 "-" "curl/7.35.0"
127.0.0.1 - - [07/Mar/2018:19:26:44 +0000] "GET /headers HTTP/1.1" 200 321 "-" "curl/7.35.0"
$ kubectl logs -f $V2_POD -c httpbin
127.0.0.1 - - [07/Mar/2018:19:26:44 +0000] "GET /headers HTTP/1.1" 200 361 "-" "curl/7.35.0"
```
### 清理
1.删除规则:
```
$ istioctl delete virtualservice httpbin
$ istioctl delete destinationrule httpbin
```
2.关闭 httpbin 服务和客户端:
```
$ kubectl delete deploy httpbin-v1 httpbin-v2 sleep
$ kubectl delete svc httpbin
```
3.如果您不打算探索任何后续任务，请参阅 Bookinfo 清理 的说明去关闭应用程序。

### Istio Service 健康检查
展示如何对 Istio service 进行健康检查。

开始之前
- 了解 Kubernetes liveness 和 readiness 探针，Istio 认证策略和双向 TLS 认证的概念。

- 具有一个安装了 Istio 的 Kubernetes 集群，但没有启用全局双向 TLS（例如按照 安装步骤 中的描述使用 istio-demo.yaml，或者在使用 Helm 时将 global.mtls.enabled 设置为 false）。

### 使用命令选项的 liveness 和 readiness 探针
在本节中，我们将展示如何在禁用双向 TLS 时配置健康检查，然后再展示在启用双向 TLS 时它的工作情况。

### 禁用双向 TLS
运行此命令以在默认 namespace 中部署 liveness：
```
$ kubectl apply -f <(istioctl kube-inject -f samples/health-check/liveness-command.yaml)
```
等待一分钟，然后检查 pod 状态
```
$ kubectl get pod
NAME                             READY     STATUS    RESTARTS   AGE
liveness-6857c8775f-zdv9r        2/2       Running   0           1m
```
‘RESTARTS’ 列中的数字 ‘0’ 表示 liveness 探针工作正常。Readiness 探针的工作方式相同，您可以相应地修改 liveness-command.yaml 以自行尝试。

### 启用双向 TLS
运行此命令以在默认 namespace 中启用 service 的双向 TLS。
```
cat <<EOF | istioctl create -f -
apiVersion: "authentication.istio.io/v1alpha1"
kind: "Policy"
metadata:
  name: "example-1"
  namespace: "default"
spec:
  peers:
  - mtls:
EOF
```
运行此命令重新部署该 service：
```
$ kubectl delete -f <(istioctl kube-inject -f @samples/health-check/liveness-command.yaml@)
$ kubectl apply -f <(istioctl kube-inject -f @samples/health-check/liveness-command.yaml@)
```
并重复上一小节中的相同步骤以验证 liveness 探针是否工作正常。

### 使用 http 请求选项的 liveness 和 readiness 探针
本节介绍了如何使用 HTTP 请求选项配置健康检查。

### 禁用双向 TLS 策略
运行此命令删除双向 TLS 策略。
```
cat <<EOF | istioctl delete -f -
apiVersion: "authentication.istio.io/v1alpha1"
kind: "Policy"
metadata:
  name: "example-1"
  namespace: "default"
spec:
  peers:
  - mtls:
EOF
```
运行此命令以在默认 namespace 中部署 liveness：
```
$ kubectl apply -f <(istioctl kube-inject -f @samples/health-check/liveness-http.yaml@)
```
等待一分钟，然后检查 pod 状态，查看 ‘RESTARTS’ 列为 ‘0’ 以确保 liveness 工作正常。

$ kubectl get pod
NAME                             READY     STATUS    RESTARTS   AGE
liveness-http-975595bb6-5b2z7c   2/2       Running   0           1m
### 启用双向 TLS 策略
运行此命令以在默认 namespace 中启用 service 的双向 TLS。
```
cat <<EOF | istioctl create -f -
apiVersion: "authentication.istio.io/v1alpha1"
kind: "Policy"
metadata:
  name: "example-1"
  namespace: "default"
spec:
  peers:
EOF
```
运行这些命令重新部署该 service：
```
$ kubectl delete -f <(istioctl kube-inject -f   samples/health-check/liveness-http.yaml)
$ kubectl apply -f <(istioctl kube-inject -f  samples/health-check/liveness-http.yaml)
```
等待一分钟，然后检查 pod 状态，查看 ‘RESTARTS’ 列为 ‘0’ 以确保 liveness 工作正常。
```
$ kubectl get pod
NAME                             READY     STATUS    RESTARTS   AGE
liveness-http-67d5db65f5-765bb   2/2       Running   0          1m
```
请注意， liveness-http 中的镜像公开了两个端口：8001 和 8002 (源代码)。在这个 deployment 中，端口 8001 提供常规通信，而端口 8002 用于 liveness 探针。由于 Istio 代理仅会拦截在 containerPort 字段中显式声明的端口，因此，无论 Istio 的双向 TLS 是否启用，到 8002 端口的流量都将绕过 Istio 代理。但是，如果我们将端口 8001 同时用于常规流量和 liveness 探针，在启用双向 TLS 时，由于 http 请求是从 Kubelet 发送的，所以不会将客户端证书发送到 liveness-http service，因此健康检查将会失效。